{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   Introduction to Deep Learning (LDA-T3114)\\n   Skeleton code for Assignment 5: Language Identification using Recurrent Architectures\\n\\n   Hande Celikkanat & Miikka Silfverberg\\n'"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "   Introduction to Deep Learning (LDA-T3114)\n",
    "   Skeleton code for Assignment 5: Language Identification using Recurrent Architectures\n",
    "\n",
    "   Hande Celikkanat & Miikka Silfverberg\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice, random, shuffle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "\n",
    "from data import read_datasets, WORD_BOUNDARY, UNK, HISTORY_SIZE\n",
    "from paths import data_dir\n",
    "\n",
    "torch.set_num_threads(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 20\n",
    "LEARNING_RATE = 0.01\n",
    "REPORT_EVERY = 10\n",
    "EMBEDDING_DIM = 30\n",
    "HIDDEN_DIM = 20\n",
    "BATCH_SIZE = 10\n",
    "N_LAYERS = 1\n",
    "\n",
    "# this is an additional parameter for the jupyter notebook skeleton code only\n",
    "# it covers for the command-line argument in the .py code\n",
    "MODEL_CHOICE = 'lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- models ---\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 character_set_size,\n",
    "                 n_layers,\n",
    "                 hidden_dim,\n",
    "                 n_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.character_set_size = character_set_size        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # WRITE CODE HERE\n",
    "        self.embeds = nn.Embedding(character_set_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim,n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # WRITE CODE HERE\n",
    "        #[sequence_len, batch_size]\n",
    "        embeds = self.embeds(inputs) #.view(len(inputs),1,-1)\n",
    "        # We recommend to use a single input for lstm layer (no special initialization of the hidden layer):\n",
    "        lstm_out, hidden = self.lstm(embeds) \n",
    "        \n",
    "        # WRITE MORE CODE HERE\n",
    "        output = self.linear(lstm_out[-1])\n",
    "        output = F.log_softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module): \n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 character_set_size,\n",
    "                 n_layers,\n",
    "                 hidden_dim,\n",
    "                 n_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.character_set_size = character_set_size        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # WRITE CODE HERE\n",
    "        self.embeds = nn.Embedding(character_set_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim,n_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # WRITE CODE HERE\n",
    "        embeds = self.embeds(inputs) #.view(len(inputs),1,-1)\n",
    "        # We recommend to use a single input for gru layer (no special initialization of the hidden layer):\n",
    "        gru_out, hidden = self.gru(embeds)\n",
    "        \n",
    "        # WRITE MORE CODE HERE\n",
    "\n",
    "        output = self.linear(gru_out[-1])\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 character_set_size,\n",
    "                 n_layers,\n",
    "                 hidden_dim,\n",
    "                 n_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.character_set_size = character_set_size        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # WRITE CODE HERE\n",
    "        self.embeds = nn.Embedding(character_set_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim,n_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # WRITE CODE HERE\n",
    "        embeds = self.embeds(inputs) #.view(len(inputs),1,-1)\n",
    "        # We recommend to use a single input for rnn layer (no special initialization of the hidden layer):\n",
    "        rnn_out, hidden = self.rnn(embeds)\n",
    "        \n",
    "        # WRITE MORE CODE HERE\n",
    "        output = self.linear(rnn_out[-1])\n",
    "        output = F.log_softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmax_length = 5\\nordered = [[1,2,3],[1,2,3,4,5],[1,2]]\\npadded = [\\n    np.pad(li, pad_width=(0, max_length-len(li)), mode='constant', constant_values=75)\\n    for li in ordered\\n]\\npadded\\n\""
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#practice padding\n",
    "'''\n",
    "max_length = 5\n",
    "ordered = [[1,2,3],[1,2,3,4,5],[1,2]]\n",
    "padded = [\n",
    "    np.pad(li, pad_width=(0, max_length-len(li)), mode='constant', constant_values=75)\n",
    "    for li in ordered\n",
    "]\n",
    "padded\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- auxilary functions ---\n",
    "def get_max_min_length(batch):\n",
    "    max_len = -9999\n",
    "    min_len = 9999\n",
    "    for word in batch:\n",
    "        length = len(word[\"WORD\"])\n",
    "        if length > max_len:\n",
    "            max_len = length\n",
    "        if length < min_len:\n",
    "            min_len = length\n",
    "        \n",
    "    return max_len,min_len\n",
    "    \n",
    "def make_words_same_length(batch, max_len):    \n",
    "    for word in batch:\n",
    "        length = len(word[\"WORD\"])\n",
    "        word[\"BALANCED\"] = torch.tensor(np.pad(word[\"TENSOR\"], pad_width=(0,max_len-length), mode='constant',constant_values=75))\n",
    "    return batch\n",
    "\n",
    "def get_minibatch(minibatchwords, character_map, languages):\n",
    "    mb_x = None\n",
    "    mb_y = None\n",
    "    \n",
    "    # WRITE CODE HERE\n",
    "    \n",
    "    # CASE 1 batch_size = 1\n",
    "    '''\n",
    "    bs = 1\n",
    "    mb_x = torch.tensor(minibatchwords[0][\"TENSOR\"])\n",
    "    mb_y = torch.tensor(label_to_idx(minibatchwords[0][\"LANGUAGE\"],languages))\n",
    "    \n",
    "    mb_x = mb_x.view(len(mb_x),bs)\n",
    "    '''\n",
    "\n",
    "    # CASE 2 batch_size > 1\n",
    "    #LABELS - list to tensor, long type\n",
    "    mb_y = torch.tensor([label_to_idx(word[\"LANGUAGE\"],languages) for word in minibatchwords],dtype=torch.long)\n",
    "    \n",
    "    #max length of the words\n",
    "    max_len, min_len = get_max_min_length(minibatchwords)\n",
    "    balanced_batch = make_words_same_length(minibatchwords,max_len)\n",
    "    \n",
    "    mb_x = [word[\"BALANCED\"] for word in balanced_batch]      \n",
    "    mb_x = torch.stack(mb_x).t()\n",
    "\n",
    "    return mb_x,mb_y\n",
    "\n",
    "def label_to_idx(lan, languages):\n",
    "    languages_ordered = list(languages)\n",
    "    languages_ordered.sort()\n",
    "    return torch.LongTensor([languages_ordered.index(lan)])\n",
    "\n",
    "\n",
    "def get_word_length(word_ex):\n",
    "    return len(word_ex['WORD'])    \n",
    "\n",
    "def evaluate(dataset,model,eval_batch_size,character_map,languages):\n",
    "    correct = 0\n",
    "    \n",
    "    # WRITE CODE HERE IF YOU LIKE\n",
    "    for i in range(0,len(dataset),eval_batch_size):\n",
    "        minibatchwords = dataset[i:i+eval_batch_size]    \n",
    "        mb_x, mb_y = get_minibatch(minibatchwords, character_map, languages)\n",
    "        \n",
    "        # WRITE CODE HERE\n",
    "        outputs = model(mb_x)\n",
    "        _,predicted= torch.max(outputs.data,1)\n",
    "        correct += (predicted == mb_y).sum()\n",
    "    return correct * 100.0 / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- initialization ---\n",
    "\n",
    "if BATCH_SIZE == 1:\n",
    "    data, character_map, languages = read_datasets('uralic.mini',data_dir)\n",
    "else:\n",
    "    data, character_map, languages = read_datasets('uralic',data_dir)\n",
    "\n",
    "trainset = [datapoint for lan in languages for datapoint in data['training'][lan]]\n",
    "n_languages = len(languages)\n",
    "character_set_size = len(character_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "\n",
    "if MODEL_CHOICE == 'lstm':\n",
    "    model = LSTMModel(embedding_dim=EMBEDDING_DIM,\n",
    "                                character_set_size = character_set_size,\n",
    "                                n_layers = N_LAYERS,\n",
    "                                hidden_dim = HIDDEN_DIM,\n",
    "                                n_classes = n_languages)\n",
    "elif MODEL_CHOICE == 'gru':\n",
    "    model = GRUModel(embedding_dim=EMBEDDING_DIM,\n",
    "                                character_set_size = character_set_size,\n",
    "                                n_layers = N_LAYERS,\n",
    "                                hidden_dim = HIDDEN_DIM,\n",
    "                                n_classes = n_languages)\n",
    "elif MODEL_CHOICE == 'rnn':\n",
    "    model = RNNModel(embedding_dim=EMBEDDING_DIM,\n",
    "                                character_set_size = character_set_size,\n",
    "                                n_layers = N_LAYERS,\n",
    "                                hidden_dim = HIDDEN_DIM,\n",
    "                                n_classes = n_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ollijokinen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 426.3727\n",
      "epoch: 2, loss: 349.3762\n",
      "epoch: 3, loss: 326.9194\n",
      "epoch: 4, loss: 306.2962\n",
      "epoch: 5, loss: 299.2069\n",
      "epoch: 6, loss: 295.4335\n",
      "epoch: 7, loss: 293.2036\n",
      "epoch: 8, loss: 278.4649\n",
      "epoch: 9, loss: 275.5501\n",
      "epoch: 10, loss: 264.3175\n",
      "epoch: 10, loss: 264.3175, train acc: 81.37%, dev acc: 72.17%\n",
      "epoch: 11, loss: 265.7043\n",
      "epoch: 12, loss: 267.8220\n",
      "epoch: 13, loss: 264.4397\n",
      "epoch: 14, loss: 262.4412\n",
      "epoch: 15, loss: 268.5876\n",
      "epoch: 16, loss: 262.6564\n",
      "epoch: 17, loss: 260.0608\n",
      "epoch: 18, loss: 256.4471\n",
      "epoch: 19, loss: 244.5830\n",
      "epoch: 20, loss: 258.0428\n",
      "epoch: 20, loss: 258.0428, train acc: 82.82%, dev acc: 68.33%\n"
     ]
    }
   ],
   "source": [
    "# --- training loop ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Generally speaking, it's a good idea to shuffle your\n",
    "    # datasets once every epoch.\n",
    "    shuffle(trainset)\n",
    "\n",
    "    # WRITE CODE HERE\n",
    "    # Sort your training set according to word-length, \n",
    "    # so that similar-length words end up near each other\n",
    "    # You can use the function get_word_length as your sort key.\n",
    "    \n",
    "    #take care of this when the batch size > 1\n",
    "    trainset = sorted(trainset, key = lambda i: len(i['WORD']))\n",
    "    \n",
    "    \n",
    "    for i in range(0,len(trainset),BATCH_SIZE):\n",
    "        minibatchwords = trainset[i:i+BATCH_SIZE]\n",
    "        mb_x, mb_y = get_minibatch(minibatchwords, character_map, languages)\n",
    "        \n",
    "        # WRITE CODE HERE\n",
    "        \n",
    "        optimizer.zero_grad()        \n",
    "        outputs = model(mb_x)\n",
    "        loss = loss_function(outputs,mb_y)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "\n",
    "    print('epoch: %d, loss: %.4f' % ((epoch+1), total_loss))\n",
    "    if ((epoch+1) % REPORT_EVERY) == 0:\n",
    "        train_acc = evaluate(trainset,model,BATCH_SIZE,character_map,languages)\n",
    "        dev_acc = evaluate(data['dev'],model,BATCH_SIZE,character_map,languages)\n",
    "        print('epoch: %d, loss: %.4f, train acc: %.2f%%, dev acc: %.2f%%' % \n",
    "              (epoch+1, total_loss, train_acc, dev_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ollijokinen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 70.25%\n"
     ]
    }
   ],
   "source": [
    "# --- test ---    \n",
    "test_acc = evaluate(data['test'],model,BATCH_SIZE,character_map,languages)        \n",
    "print('test acc: %.2f%%' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
