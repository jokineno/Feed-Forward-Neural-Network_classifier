{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cX61QTz_WACp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   Introduction to Deep Learning (LDA-T3114)\\n   Skeleton code for Assignment 1: Training Linear Classifiers\\n\\n   Hande Celikkanat & Miikka Silfverberg\\n'"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "   Introduction to Deep Learning (LDA-T3114)\n",
    "   Skeleton code for Assignment 1: Training Linear Classifiers\n",
    "\n",
    "   Hande Celikkanat & Miikka Silfverberg\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWozARu6Wqff"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTgWmDBDWutR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ollijokinen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#ATTENTION: If necessary, add the paths to your data_semeval.py and paths.py here:\n",
    "#import sys\n",
    "#sys.path.append('</path/to/below/modules>')\n",
    "from data_semeval import *\n",
    "from paths import data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJp3BkaHWw74"
   },
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "\n",
    "n_classes = len(LABEL_INDICES)\n",
    "n_epochs = 30\n",
    "learning_rate = 0.05\n",
    "report_every = 1\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gvAm0fHW2lH"
   },
   "outputs": [],
   "source": [
    "def make_bow(tweet, indices):\n",
    "    feature_ids = list(indices[tok] for tok in tweet['BODY'] if tok in indices)\n",
    "    bow_vec = torch.zeros(len(indices))\n",
    "    bow_vec[feature_ids] = 1\n",
    "    return bow_vec.view(1, -1)\n",
    "\n",
    "def generate_bow_representations(data):\n",
    "    vocab = set(token for tweet in data['training'] for token in tweet['BODY'])\n",
    "    vocab_size = len(vocab) \n",
    "    indices = {w:i for i, w in enumerate(vocab)}\n",
    "  \n",
    "    for split in [\"training\",\"development.input\",\"development.gold\",\"test.input\",\"test.gold\"]:\n",
    "        for tweet in data[split]:\n",
    "            tweet['BOW'] = make_bow(tweet,indices)\n",
    "\n",
    "    return indices, vocab_size\n",
    "\n",
    "# Convert string label to pytorch format.\n",
    "def label_to_idx(label):\n",
    "    return torch.LongTensor([LABEL_INDICES[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQU98XHwW50h"
   },
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        # WRITE CODE HERE\n",
    "        self.linear = torch.nn.Linear(vocab_size,n_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "      # WRITE CODE HERE\n",
    "        output = F.log_softmax(self.linear(input))\n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiXMR3LAW_GC"
   },
   "outputs": [],
   "source": [
    "#--- data loading ---\n",
    "data = read_semeval_datasets(data_dir)\n",
    "indices, vocab_size = generate_bow_representations(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0k4wWf8XCMr"
   },
   "outputs": [],
   "source": [
    "#--- set up ---\n",
    "model = LogisticRegression(vocab_size, n_classes)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22031"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAq0EdZIXJo5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ollijokinen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.8298\n",
      "epoch: 1, loss: 0.5866\n",
      "epoch: 2, loss: 0.4695\n",
      "epoch: 3, loss: 0.3927\n",
      "epoch: 4, loss: 0.3374\n",
      "epoch: 5, loss: 0.2955\n",
      "epoch: 6, loss: 0.2625\n",
      "epoch: 7, loss: 0.2360\n",
      "epoch: 8, loss: 0.2142\n",
      "epoch: 9, loss: 0.1959\n",
      "epoch: 10, loss: 0.1804\n",
      "epoch: 11, loss: 0.1671\n",
      "epoch: 12, loss: 0.1555\n",
      "epoch: 13, loss: 0.1454\n",
      "epoch: 14, loss: 0.1364\n",
      "epoch: 15, loss: 0.1285\n",
      "epoch: 16, loss: 0.1214\n",
      "epoch: 17, loss: 0.1150\n",
      "epoch: 18, loss: 0.1092\n",
      "epoch: 19, loss: 0.1040\n",
      "epoch: 20, loss: 0.0992\n",
      "epoch: 21, loss: 0.0949\n",
      "epoch: 22, loss: 0.0909\n",
      "epoch: 23, loss: 0.0872\n",
      "epoch: 24, loss: 0.0838\n",
      "epoch: 25, loss: 0.0807\n",
      "epoch: 26, loss: 0.0778\n",
      "epoch: 27, loss: 0.0751\n",
      "epoch: 28, loss: 0.0726\n",
      "epoch: 29, loss: 0.0702\n"
     ]
    }
   ],
   "source": [
    "#--- training ---\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for tweet in data['training']:  \n",
    "        gold_class = label_to_idx(tweet['SENTIMENT'])\n",
    "        #WRITE CODE HERE   \n",
    "        x = tweet[\"BOW\"]\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(x) \n",
    "        loss = criterion(outputs, gold_class) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        total_loss += loss\n",
    "        \n",
    "    if ((epoch+1) % report_every) == 0:\n",
    "        print('epoch: %d, loss: %.4f' % (epoch, total_loss/len(data['training'])))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "47CYoN-RXLfl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ollijokinen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 43.99\n"
     ]
    }
   ],
   "source": [
    "#--- test ---\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for tweet in data['test.gold']:\n",
    "        gold_class = label_to_idx(tweet['SENTIMENT'])\n",
    "\n",
    "        # WRITE CODE HERE\n",
    "        y = tweet[\"BOW\"] #tensor\n",
    "        outputs = model(y)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        correct += torch.eq(predicted,gold_class).item()\n",
    "        \n",
    "\n",
    "        if verbose:\n",
    "            print('TEST DATA: %s, OUTPUT: %s, GOLD LABEL: %d' % \n",
    "                (tweet['BODY'], tweet['SENTIMENT'], predicted))\n",
    "\n",
    "print('test accuracy: %.2f' % (100.0 * correct / len(data['test.gold'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST ACCURACY: 65.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL_hw1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
