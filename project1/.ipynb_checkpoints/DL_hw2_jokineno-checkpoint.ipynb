{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   Introduction to Deep Learning (LDA-T3114)\\n   Skeleton code for Assignment 2: Sentiment Classification on a Feed-Forward Neural Network using Pretrained Embeddings\\n   Feel free to change this code according to your design!\\n   Remember to use PyTorch for your NN implementation.\\n\\n   Hande Celikkanat & Miikka Silfverberg\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "   Introduction to Deep Learning (LDA-T3114)\n",
    "   Skeleton code for Assignment 2: Sentiment Classification on a Feed-Forward Neural Network using Pretrained Embeddings\n",
    "   Feel free to change this code according to your design!\n",
    "   Remember to use PyTorch for your NN implementation.\n",
    "\n",
    "   Hande Celikkanat & Miikka Silfverberg\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to these data manipulation files if necessary:\n",
    "# import sys\n",
    "# sys.path.append('</PATH/TO/DATA/MANIP/FILES>')\n",
    "from data_semeval import *\n",
    "from paths import data_dir, model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the embeddings file to use\n",
    "# Alternatively, you can also use the text file GoogleNews-pruned2tweets.txt (from Moodle),\n",
    "# or the full set, wiz. GoogleNews-vectors-negative300.bin (from https://code.google.com/archive/p/word2vec/) \n",
    "embeddings_file = 'GoogleNews-pruned2tweets.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "\n",
    "# Feel free to experiment with different hyperparameters to see how they compare! \n",
    "# You can turn in your assignment with the best settings you find.\n",
    "\n",
    "n_classes = len(LABEL_INDICES)\n",
    "n_epochs = 30 \n",
    "learning_rate = 0.001\n",
    "report_every = 1\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- auxilary functions ---\n",
    "\n",
    "# To convert string label to pytorch format:\n",
    "def label_to_idx(label):\n",
    "    return torch.LongTensor([LABEL_INDICES[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    # Feel free to add whichever arguments you like here.\n",
    "    # Note that pretrained_embeds is a numpy matrix of shape (num_embeddings, embedding_dim)\n",
    "    def __init__(self, pretrained_embeds, n_classes):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(pretrained_embeds),len(pretrained_embeds[0]))\n",
    "        self.linear1 = nn.Linear(len(pretrained_embeds[0]),200)\n",
    "        self.linear2 = nn.Linear(200, n_classes)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # WRITE CODE HERE\n",
    "        out = F.relu(self.linear1(x).view(1,-1))\n",
    "        out = F.relu(self.linear2(out))\n",
    "       \n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- data loading ---\n",
    "data = read_semeval_datasets(data_dir)\n",
    "gensim_embeds = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(model_dir, embeddings_file), binary=True)\n",
    "pretrained_embeds = gensim_embeds.vectors\n",
    "# To convert words in the input tweet to indices of the embeddings matrix:\n",
    "word_to_idx = {word: i for i, word in enumerate(gensim_embeds.vocab.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- set up ---\n",
    "# WRITE CODE HERE\n",
    "\n",
    "model = FFNN(pretrained_embeds, n_classes)\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_vectors(indices,pretrained_embeds):\n",
    "    word_vectors = pretrained_embeds[indices]\n",
    "    return word_vectors\n",
    "\n",
    "def average_vectors(word_vectors):\n",
    "    sumvector = np.ones((1,300))\n",
    "    distributor = word_vectors.shape[0]\n",
    "    for pretrained_vector in word_vectors:\n",
    "        sumvector += pretrained_vector\n",
    "    cbow_vector = sumvector\n",
    "    return cbow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_CBOW(data,pretrained_embeds,word_to_idx):\n",
    "    for split in [\"training\",\"test.gold\"]:#,\"development.input\",\"development.gold\",\"test.input\",\"test.gold]:\n",
    "        for tweet in data[split]:\n",
    "            text = tweet[\"BODY\"]\n",
    "            indices = [word_to_idx[w] for w in text if w in word_to_idx]\n",
    "            vectors = indices_to_vectors(indices,pretrained_embeds)\n",
    "            tweet[\"CBOW\"] = average_vectors(vectors)\n",
    "\n",
    "tweet_to_CBOW(data,pretrained_embeds,word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ollijokinen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 108.7544\n",
      "epoch: 1, loss: 87.8302\n",
      "epoch: 2, loss: 81.6534\n",
      "epoch: 3, loss: 79.7272\n",
      "epoch: 4, loss: 78.1084\n",
      "epoch: 5, loss: 76.3368\n",
      "epoch: 6, loss: 74.5814\n",
      "epoch: 7, loss: 73.8837\n",
      "epoch: 8, loss: 72.8829\n",
      "epoch: 9, loss: 73.1382\n",
      "epoch: 10, loss: 70.8475\n",
      "epoch: 11, loss: 70.2245\n",
      "epoch: 12, loss: 70.4095\n",
      "epoch: 13, loss: 68.6214\n",
      "epoch: 14, loss: 68.2592\n",
      "epoch: 15, loss: 67.9911\n",
      "epoch: 16, loss: 65.9797\n",
      "epoch: 17, loss: 65.7192\n",
      "epoch: 18, loss: 65.6712\n",
      "epoch: 19, loss: 63.7827\n",
      "epoch: 20, loss: 64.5290\n",
      "epoch: 21, loss: 61.8365\n",
      "epoch: 22, loss: 61.2123\n",
      "epoch: 23, loss: 60.4831\n",
      "epoch: 24, loss: 60.2740\n",
      "epoch: 25, loss: 59.4426\n",
      "epoch: 26, loss: 58.9275\n",
      "epoch: 27, loss: 58.7758\n",
      "epoch: 28, loss: 56.8457\n",
      "epoch: 29, loss: 56.9683\n"
     ]
    }
   ],
   "source": [
    "#--- training ---\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for tweet in data['training']:  \n",
    "        gold_class = label_to_idx(tweet['SENTIMENT']) #real class\n",
    "        gold_class = torch.tensor(gold_class,dtype=torch.long)\n",
    "\n",
    "        # WRITE CODE HERE\n",
    "        \n",
    "        text = tweet[\"CBOW\"]\n",
    "        text = torch.tensor(text,dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(text) #predict\n",
    "        loss = loss_function(log_probs, gold_class) #count loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        \n",
    "    if ((epoch+1) % report_every) == 0:\n",
    "        print('epoch: %d, loss: %.4f' % (epoch, total_loss*100/len(data['training'])))\n",
    "    \n",
    "# Feel free to use the development data to tune hyperparameters if you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 62.72\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for tweet in data['test.gold']:\n",
    "        gold_class = label_to_idx(tweet['SENTIMENT'])\n",
    "        # WRITE CODE HERE\n",
    "        cbow = torch.tensor(tweet[\"CBOW\"], dtype=torch.float)\n",
    "        log_probs = model(cbow)\n",
    "        _,predicted = torch.max(log_probs.data,1)\n",
    "        \n",
    "        correct += torch.eq(predicted,gold_class).item()\n",
    "\n",
    "        \n",
    "        if verbose:\n",
    "            print('TEST DATA: %s, OUTPUT: %s, GOLD LABEL: %d' % (tweet['BODY'], tweet['SENTIMENT'], predicted))\n",
    "        \n",
    "    print('test accuracy: %.2f' % (100.0 * correct / len(data['test.gold'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I played around with different parameters. I got a result around 65% and the most of the times in my experiments the result got lower. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
