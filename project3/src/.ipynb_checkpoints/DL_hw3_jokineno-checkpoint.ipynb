{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   Introduction to Deep Learning (LDA-T3114)\\n   Skeleton code for Assignment 3: Language Identification for Uralic Languages\\n\\n   Hande Celikkanat & Miikka Silfverberg\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "   Introduction to Deep Learning (LDA-T3114)\n",
    "   Skeleton code for Assignment 3: Language Identification for Uralic Languages\n",
    "\n",
    "   Hande Celikkanat & Miikka Silfverberg\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice, random, shuffle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import read_datasets, WORD_BOUNDARY, UNK, HISTORY_SIZE\n",
    "\n",
    "from paths import data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 1500\n",
    "LEARNING_RATE = 0.1\n",
    "REPORT_EVERY = 100\n",
    "VERBOSE = False\n",
    "EMBEDDING_DIM=30\n",
    "HIDDEN_DIM=50\n",
    "BATCH_SIZE=100 # raise from 1 to 10\n",
    "N_LAYERS=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 history_size, \n",
    "                 hidden_dim, \n",
    "                 n_layers,\n",
    "                 character_set_size):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.history_size = history_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embed = nn.Embedding(character_set_size,embedding_dim)\n",
    "        # WRITE CODE HERE\n",
    "        self.linear1 = nn.Linear(history_size*embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim,character_set_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # WRITE CODE HERE\n",
    "        input_x = inputs.shape[0]\n",
    "        embeds = self.embed(inputs).view(input_x,120)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- auxilary functions ---\n",
    "def get_ll(word_ex,model,history_size,character_map):\n",
    "    with torch.no_grad():        \n",
    "        char_tuples = word_ex['TUPLES']\n",
    "        # WRITE CODE HERE \n",
    "        \n",
    "        total = 0\n",
    "        \n",
    "        #------Case 2----\n",
    "        inputs = torch.cat([i for i,j in char_tuples],dim=0)\n",
    "        classes = torch.cat([j for i,j in char_tuples],dim=0)\n",
    "        \n",
    "        inputs = inputs.reshape(int(inputs.shape[0]/history_size),history_size)\n",
    "        \n",
    "        result = model(inputs)\n",
    "        x = 0\n",
    "        for i in result:\n",
    "            #get an index of the goal\n",
    "            goal_index = classes[x]\n",
    "            \n",
    "            #add a predicted goal_value to total\n",
    "            total += i[goal_index]\n",
    "            x+=1\n",
    "        \n",
    "        return total\n",
    "    \n",
    "        #for a single word\n",
    "         #------Case 1----\n",
    "        \"\"\"\n",
    "        for tup in char_tuples:\n",
    "            result = model(tup[0])\n",
    "            gold = tup[1].item()\n",
    "            \n",
    "            value = result[0][gold]\n",
    "           \n",
    "            total+=value\n",
    "            \n",
    "        return total\n",
    "        \"\"\"\n",
    "    \n",
    "def guess_language(word_ex,models,history_size,character_map):\n",
    "    lls = [(lan,get_ll(word_ex,models[lan],history_size,character_map)) \n",
    "           for lan in models]\n",
    "    #choose the language that has the highest value\n",
    "    return max(lls,key=lambda x: x[1])[0]\n",
    "\n",
    "def evaluate(dataset,models,HISTORY_SIZE,character_map):\n",
    "    corr = 0\n",
    "    for word_ex in dataset:\n",
    "        sys_lan = guess_language(word_ex,\n",
    "                                 models,\n",
    "                                 HISTORY_SIZE,\n",
    "                                 character_map)\n",
    "        \n",
    "        #if the guess is right then correct +1\n",
    "        if sys_lan == word_ex['LANGUAGE']:\n",
    "            corr += 1\n",
    "    return corr * 100.0 / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- initialization ---\n",
    "data, character_map, languages = read_datasets('uralic',data_dir)\n",
    "# We initialize one language model for each language.\n",
    "models = {lan:LanguageModel(EMBEDDING_DIM,\n",
    "                            HISTORY_SIZE,\n",
    "                            HIDDEN_DIM,\n",
    "                            N_LAYERS,\n",
    "                            len(character_map)) for lan in languages}\n",
    "# Each language model requires its own optimizer.\n",
    "optimizers = {lan:optim.SGD(models[lan].parameters(), LEARNING_RATE) \n",
    "              for lan in languages}\n",
    "# We can use the same loss function for training all language models.\n",
    "loss_function = nn.NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, loss: 9.6846, dev acc: 76.75%\n",
      "epoch: 200, loss: 10.0040, dev acc: 75.83%\n",
      "epoch: 300, loss: 9.8815, dev acc: 76.42%\n",
      "epoch: 400, loss: 9.1840, dev acc: 76.92%\n",
      "epoch: 500, loss: 10.0524, dev acc: 76.75%\n",
      "epoch: 600, loss: 9.7608, dev acc: 76.75%\n",
      "epoch: 700, loss: 9.3985, dev acc: 76.42%\n",
      "epoch: 800, loss: 9.4772, dev acc: 76.58%\n",
      "epoch: 900, loss: 9.5494, dev acc: 77.00%\n",
      "epoch: 1000, loss: 8.6998, dev acc: 75.17%\n",
      "epoch: 1100, loss: 9.0632, dev acc: 74.67%\n",
      "epoch: 1200, loss: 9.7724, dev acc: 75.42%\n",
      "epoch: 1300, loss: 8.5290, dev acc: 76.17%\n",
      "epoch: 1400, loss: 8.8916, dev acc: 74.50%\n",
      "epoch: 1500, loss: 8.2444, dev acc: 76.00%\n"
     ]
    }
   ],
   "source": [
    "#--- training ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0\n",
    "    for lan in data['training']:  \n",
    "        trainset = data['training'][lan]\n",
    "        optimizer = optimizers[lan]\n",
    "        model = models[lan]\n",
    "        # Generally speaking, it's a good idea to shuffle your\n",
    "        # training sets once every epoch. This serves as an\n",
    "        # approximation of drawing training examples at random\n",
    "        # from the training set.\n",
    "        shuffle(trainset)\n",
    "\n",
    "        for i in range(0,int(len(trainset)/BATCH_SIZE),BATCH_SIZE):\n",
    "            minibatchwords = trainset[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "            minibatch = [choice(word_ex['TUPLES']) \n",
    "                         for word_ex in minibatchwords]\n",
    "            \n",
    "            # WRITE CODE HERE\n",
    "           \n",
    "            #------CASE1-------, minibatch size = 1\n",
    "            \n",
    "            #predicting_letters = torch.tensor(minibatch[0][0],dtype=torch.long)\n",
    "            #following_letter = torch.tensor(minibatch[0][1],dtype=torch.long)\n",
    "            \n",
    "            \n",
    "            #-----CASE 2--------, minibatch size = 10\n",
    "            \n",
    "            #create a tensor size 10x4 and goal tensor 10x1\n",
    "            inputvectors = [i for i,j in minibatch]\n",
    "            classes = [j for i,j in minibatch]\n",
    "            mb_x = torch.cat(inputvectors,dim=0).view(BATCH_SIZE,4)\n",
    "            following_letter = torch.cat(classes,dim=0)\n",
    "            \n",
    "            #sanity check:\n",
    "            #print(mb_x)\n",
    "            #print(following_letter.shape)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            log_probs = model(mb_x) #predict\n",
    "            \n",
    "            loss = loss_function(log_probs, following_letter) #count loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss\n",
    "    \n",
    "    if ((epoch+1) % REPORT_EVERY) == 0:\n",
    "        \n",
    "        acc = evaluate(data['dev'],models,HISTORY_SIZE,character_map)\n",
    "        print('epoch: %d, loss: %.4f, dev acc: %.2f%%' % \n",
    "              (epoch+1, total_loss, acc))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 75.08%\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(data['test'],models,HISTORY_SIZE,character_map)        \n",
    "print('test acc: %.2f%%' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy up to 75-76% with batch size 100, n_epochs = 1500, report_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
