{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from plotting import plot_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 100\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0e1dc9e40547289e6072c2867fc7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# --- CIFAR initialization ---\n",
    "\n",
    "# We transform torchvision.datasets.CIFAR10 outputs to tensors\n",
    "# Plus, we add a random horizontal transformation to the training data\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=train_transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Create Pytorch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE_TEST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    print(\"img\",img)\n",
    "    img = img / 2 + 0.5\n",
    "    print(\"img2\", img)\n",
    "    # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 32, 32])\n",
      "tensor([2, 7, 7, 3, 9, 1, 8, 4, 4, 8, 6, 3, 7, 5, 7, 3, 3, 9, 5, 6, 9, 4, 4, 2,\n",
      "        6, 5, 1, 9, 6, 3, 8, 2, 1, 5, 4, 3, 3, 3, 5, 3, 8, 0, 6, 8, 0, 4, 6, 2,\n",
      "        5, 4, 5, 6, 3, 5, 5, 8, 0, 2, 0, 3, 0, 2, 5, 4, 8, 3, 1, 7, 8, 3, 6, 2,\n",
      "        5, 4, 0, 2, 8, 5, 7, 2, 3, 5, 6, 9, 1, 2, 5, 4, 6, 5, 6, 0, 0, 3, 5, 0,\n",
      "        9, 9, 3, 5])\n",
      "img tensor([[[0.7725, 0.7804, 0.7765,  ..., 0.7333, 0.7333, 0.7490],\n",
      "         [0.7804, 0.7882, 0.7843,  ..., 0.7490, 0.7451, 0.7608],\n",
      "         [0.7765, 0.7804, 0.7804,  ..., 0.7490, 0.7451, 0.7608],\n",
      "         ...,\n",
      "         [0.7882, 0.7882, 0.7647,  ..., 0.7686, 0.7686, 0.7216],\n",
      "         [0.7843, 0.7804, 0.7569,  ..., 0.7647, 0.7686, 0.7804],\n",
      "         [0.7843, 0.7725, 0.7451,  ..., 0.7412, 0.7373, 0.7569]],\n",
      "\n",
      "        [[0.7176, 0.7255, 0.7216,  ..., 0.6745, 0.6745, 0.6902],\n",
      "         [0.7255, 0.7333, 0.7294,  ..., 0.6902, 0.6863, 0.6980],\n",
      "         [0.7216, 0.7255, 0.7255,  ..., 0.6902, 0.6863, 0.7020],\n",
      "         ...,\n",
      "         [0.7216, 0.7216, 0.6980,  ..., 0.7098, 0.7137, 0.6627],\n",
      "         [0.7176, 0.7137, 0.6902,  ..., 0.7098, 0.7137, 0.7255],\n",
      "         [0.7176, 0.7059, 0.6824,  ..., 0.6902, 0.6863, 0.7020]],\n",
      "\n",
      "        [[0.6039, 0.6118, 0.6078,  ..., 0.5647, 0.5647, 0.5765],\n",
      "         [0.6118, 0.6196, 0.6157,  ..., 0.5725, 0.5725, 0.5843],\n",
      "         [0.6078, 0.6118, 0.6078,  ..., 0.5765, 0.5725, 0.5843],\n",
      "         ...,\n",
      "         [0.6118, 0.6118, 0.5882,  ..., 0.5765, 0.5804, 0.5255],\n",
      "         [0.6078, 0.6039, 0.5804,  ..., 0.5725, 0.5765, 0.5843],\n",
      "         [0.5961, 0.5922, 0.5725,  ..., 0.5647, 0.5608, 0.5765]]])\n",
      "img2 tensor([[[0.8863, 0.8902, 0.8882,  ..., 0.8667, 0.8667, 0.8745],\n",
      "         [0.8902, 0.8941, 0.8922,  ..., 0.8745, 0.8725, 0.8804],\n",
      "         [0.8882, 0.8902, 0.8902,  ..., 0.8745, 0.8725, 0.8804],\n",
      "         ...,\n",
      "         [0.8941, 0.8941, 0.8824,  ..., 0.8843, 0.8843, 0.8608],\n",
      "         [0.8922, 0.8902, 0.8784,  ..., 0.8824, 0.8843, 0.8902],\n",
      "         [0.8922, 0.8863, 0.8725,  ..., 0.8706, 0.8686, 0.8784]],\n",
      "\n",
      "        [[0.8588, 0.8627, 0.8608,  ..., 0.8373, 0.8373, 0.8451],\n",
      "         [0.8627, 0.8667, 0.8647,  ..., 0.8451, 0.8431, 0.8490],\n",
      "         [0.8608, 0.8627, 0.8627,  ..., 0.8451, 0.8431, 0.8510],\n",
      "         ...,\n",
      "         [0.8608, 0.8608, 0.8490,  ..., 0.8549, 0.8569, 0.8314],\n",
      "         [0.8588, 0.8569, 0.8451,  ..., 0.8549, 0.8569, 0.8627],\n",
      "         [0.8588, 0.8529, 0.8412,  ..., 0.8451, 0.8431, 0.8510]],\n",
      "\n",
      "        [[0.8020, 0.8059, 0.8039,  ..., 0.7824, 0.7824, 0.7882],\n",
      "         [0.8059, 0.8098, 0.8078,  ..., 0.7863, 0.7863, 0.7922],\n",
      "         [0.8039, 0.8059, 0.8039,  ..., 0.7882, 0.7863, 0.7922],\n",
      "         ...,\n",
      "         [0.8059, 0.8059, 0.7941,  ..., 0.7882, 0.7902, 0.7627],\n",
      "         [0.8039, 0.8020, 0.7902,  ..., 0.7863, 0.7882, 0.7922],\n",
      "         [0.7980, 0.7961, 0.7863,  ..., 0.7824, 0.7804, 0.7882]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for (i,j) in train_loader:\n",
    "    print(i.shape) # 100 x 3 x 32 x 32\n",
    "    print(j)\n",
    "    kuva = i[0]\n",
    "    imshow(kuva)\n",
    "    break\n",
    "    \n",
    "# data is ok! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing CNN \n",
    "\n",
    "- The input:  BATCH SIZE TRAIN x NUM CHANNELS x WIDTH x HEIGHT,\n",
    "    - where NUM CHANNELS=3 (R-G-B channels), and WIDTH=HEIGHT=32 (pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Please make sure your model includes at least two convolutional layers, followed by suitable non- linear functions and max pooling layers. Since there will be a rather large number of layers, please consider also to organize these layers into an torch.nn.Sequential module, which may result in cleaner code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        # WRITE CODE HERE\n",
    "        \n",
    "        # convolution 1 \n",
    "        #non linear: relu, tanh, sigmoid\n",
    "        # max pooling\n",
    "        \n",
    "        # convolution 2\n",
    "        #non linear\n",
    "        #max pooling\n",
    "        \n",
    "        #esimerkiss√§ input kuvat on 28*28\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(8 * 8 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # WRITE CODE HERE\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Batch 0/500: Loss: 2.3264 | Train Acc: 16.000% (16/100)\n",
      "Epoch 0 - Batch 1/500: Loss: 6.0502 | Train Acc: 17.000% (34/200)\n",
      "Epoch 0 - Batch 2/500: Loss: 8.1193 | Train Acc: 17.667% (53/300)\n",
      "Epoch 0 - Batch 3/500: Loss: 9.7890 | Train Acc: 14.750% (59/400)\n",
      "Epoch 0 - Batch 4/500: Loss: 9.6798 | Train Acc: 15.000% (75/500)\n",
      "Epoch 0 - Batch 5/500: Loss: 9.1469 | Train Acc: 15.000% (90/600)\n",
      "Epoch 0 - Batch 6/500: Loss: 9.1922 | Train Acc: 15.429% (108/700)\n",
      "Epoch 0 - Batch 7/500: Loss: 9.1601 | Train Acc: 15.750% (126/800)\n",
      "Epoch 0 - Batch 8/500: Loss: 8.8891 | Train Acc: 16.000% (144/900)\n",
      "Epoch 0 - Batch 9/500: Loss: 8.5854 | Train Acc: 16.400% (164/1000)\n",
      "Epoch 0 - Batch 10/500: Loss: 8.2362 | Train Acc: 17.182% (189/1100)\n",
      "Epoch 0 - Batch 11/500: Loss: 7.9776 | Train Acc: 17.500% (210/1200)\n",
      "Epoch 0 - Batch 12/500: Loss: 7.7576 | Train Acc: 17.308% (225/1300)\n",
      "Epoch 0 - Batch 13/500: Loss: 7.5066 | Train Acc: 17.500% (245/1400)\n",
      "Epoch 0 - Batch 14/500: Loss: 7.2410 | Train Acc: 17.933% (269/1500)\n",
      "Epoch 0 - Batch 15/500: Loss: 7.0336 | Train Acc: 17.750% (284/1600)\n",
      "Epoch 0 - Batch 16/500: Loss: 6.8090 | Train Acc: 17.529% (298/1700)\n",
      "Epoch 0 - Batch 17/500: Loss: 6.6125 | Train Acc: 17.833% (321/1800)\n",
      "Epoch 0 - Batch 18/500: Loss: 6.4731 | Train Acc: 17.579% (334/1900)\n",
      "Epoch 0 - Batch 19/500: Loss: 6.3099 | Train Acc: 17.750% (355/2000)\n",
      "Epoch 0 - Batch 20/500: Loss: 6.1570 | Train Acc: 18.000% (378/2100)\n",
      "Epoch 0 - Batch 21/500: Loss: 5.9880 | Train Acc: 18.455% (406/2200)\n",
      "Epoch 0 - Batch 22/500: Loss: 5.8459 | Train Acc: 18.565% (427/2300)\n",
      "Epoch 0 - Batch 23/500: Loss: 5.7486 | Train Acc: 18.542% (445/2400)\n",
      "Epoch 0 - Batch 24/500: Loss: 5.6444 | Train Acc: 18.760% (469/2500)\n",
      "Epoch 0 - Batch 25/500: Loss: 5.5351 | Train Acc: 19.231% (500/2600)\n",
      "Epoch 0 - Batch 26/500: Loss: 5.4365 | Train Acc: 19.556% (528/2700)\n",
      "Epoch 0 - Batch 27/500: Loss: 5.3266 | Train Acc: 19.857% (556/2800)\n",
      "Epoch 0 - Batch 28/500: Loss: 5.2150 | Train Acc: 20.345% (590/2900)\n",
      "Epoch 0 - Batch 29/500: Loss: 5.1333 | Train Acc: 20.500% (615/3000)\n",
      "Epoch 0 - Batch 30/500: Loss: 5.0559 | Train Acc: 20.387% (632/3100)\n",
      "Epoch 0 - Batch 31/500: Loss: 4.9752 | Train Acc: 20.594% (659/3200)\n",
      "Epoch 0 - Batch 32/500: Loss: 4.9020 | Train Acc: 20.515% (677/3300)\n",
      "Epoch 0 - Batch 33/500: Loss: 4.8301 | Train Acc: 20.588% (700/3400)\n",
      "Epoch 0 - Batch 34/500: Loss: 4.7549 | Train Acc: 20.686% (724/3500)\n",
      "Epoch 0 - Batch 35/500: Loss: 4.6874 | Train Acc: 20.889% (752/3600)\n",
      "Epoch 0 - Batch 36/500: Loss: 4.6280 | Train Acc: 21.081% (780/3700)\n",
      "Epoch 0 - Batch 37/500: Loss: 4.5686 | Train Acc: 21.237% (807/3800)\n",
      "Epoch 0 - Batch 38/500: Loss: 4.5139 | Train Acc: 21.308% (831/3900)\n",
      "Epoch 0 - Batch 39/500: Loss: 4.4537 | Train Acc: 21.500% (860/4000)\n",
      "Epoch 0 - Batch 40/500: Loss: 4.3982 | Train Acc: 21.732% (891/4100)\n",
      "Epoch 0 - Batch 41/500: Loss: 4.3418 | Train Acc: 21.929% (921/4200)\n",
      "Epoch 0 - Batch 42/500: Loss: 4.2925 | Train Acc: 22.140% (952/4300)\n",
      "Epoch 0 - Batch 43/500: Loss: 4.2418 | Train Acc: 22.295% (981/4400)\n",
      "Epoch 0 - Batch 44/500: Loss: 4.1998 | Train Acc: 22.489% (1012/4500)\n",
      "Epoch 0 - Batch 45/500: Loss: 4.1550 | Train Acc: 22.717% (1045/4600)\n",
      "Epoch 0 - Batch 46/500: Loss: 4.1074 | Train Acc: 22.936% (1078/4700)\n",
      "Epoch 0 - Batch 47/500: Loss: 4.0591 | Train Acc: 23.167% (1112/4800)\n",
      "Epoch 0 - Batch 48/500: Loss: 4.0223 | Train Acc: 23.367% (1145/4900)\n",
      "Epoch 0 - Batch 49/500: Loss: 3.9849 | Train Acc: 23.440% (1172/5000)\n",
      "Epoch 0 - Batch 50/500: Loss: 3.9515 | Train Acc: 23.373% (1192/5100)\n",
      "Epoch 0 - Batch 51/500: Loss: 3.9147 | Train Acc: 23.481% (1221/5200)\n",
      "Epoch 0 - Batch 52/500: Loss: 3.8770 | Train Acc: 23.698% (1256/5300)\n",
      "Epoch 0 - Batch 53/500: Loss: 3.8377 | Train Acc: 23.889% (1290/5400)\n",
      "Epoch 0 - Batch 54/500: Loss: 3.8020 | Train Acc: 24.164% (1329/5500)\n",
      "Epoch 0 - Batch 55/500: Loss: 3.7672 | Train Acc: 24.429% (1368/5600)\n",
      "Epoch 0 - Batch 56/500: Loss: 3.7382 | Train Acc: 24.561% (1400/5700)\n",
      "Epoch 0 - Batch 57/500: Loss: 3.7065 | Train Acc: 24.776% (1437/5800)\n",
      "Epoch 0 - Batch 58/500: Loss: 3.6758 | Train Acc: 24.966% (1473/5900)\n",
      "Epoch 0 - Batch 59/500: Loss: 3.6446 | Train Acc: 25.133% (1508/6000)\n",
      "Epoch 0 - Batch 60/500: Loss: 3.6115 | Train Acc: 25.361% (1547/6100)\n",
      "Epoch 0 - Batch 61/500: Loss: 3.5870 | Train Acc: 25.371% (1573/6200)\n",
      "Epoch 0 - Batch 62/500: Loss: 3.5622 | Train Acc: 25.492% (1606/6300)\n",
      "Epoch 0 - Batch 63/500: Loss: 3.5355 | Train Acc: 25.703% (1645/6400)\n",
      "Epoch 0 - Batch 64/500: Loss: 3.5133 | Train Acc: 25.862% (1681/6500)\n",
      "Epoch 0 - Batch 65/500: Loss: 3.4922 | Train Acc: 25.894% (1709/6600)\n",
      "Epoch 0 - Batch 66/500: Loss: 3.4689 | Train Acc: 25.985% (1741/6700)\n",
      "Epoch 0 - Batch 67/500: Loss: 3.4452 | Train Acc: 26.044% (1771/6800)\n",
      "Epoch 0 - Batch 68/500: Loss: 3.4205 | Train Acc: 26.261% (1812/6900)\n",
      "Epoch 0 - Batch 69/500: Loss: 3.3974 | Train Acc: 26.443% (1851/7000)\n",
      "Epoch 0 - Batch 70/500: Loss: 3.3734 | Train Acc: 26.620% (1890/7100)\n",
      "Epoch 0 - Batch 71/500: Loss: 3.3544 | Train Acc: 26.708% (1923/7200)\n",
      "Epoch 0 - Batch 72/500: Loss: 3.3351 | Train Acc: 26.781% (1955/7300)\n",
      "Epoch 0 - Batch 73/500: Loss: 3.3169 | Train Acc: 26.824% (1985/7400)\n",
      "Epoch 0 - Batch 74/500: Loss: 3.2966 | Train Acc: 27.000% (2025/7500)\n",
      "Epoch 0 - Batch 75/500: Loss: 3.2770 | Train Acc: 27.066% (2057/7600)\n",
      "Epoch 0 - Batch 76/500: Loss: 3.2578 | Train Acc: 27.169% (2092/7700)\n",
      "Epoch 0 - Batch 77/500: Loss: 3.2406 | Train Acc: 27.269% (2127/7800)\n",
      "Epoch 0 - Batch 78/500: Loss: 3.2210 | Train Acc: 27.342% (2160/7900)\n",
      "Epoch 0 - Batch 79/500: Loss: 3.2043 | Train Acc: 27.337% (2187/8000)\n",
      "Epoch 0 - Batch 80/500: Loss: 3.1899 | Train Acc: 27.395% (2219/8100)\n",
      "Epoch 0 - Batch 81/500: Loss: 3.1715 | Train Acc: 27.512% (2256/8200)\n",
      "Epoch 0 - Batch 82/500: Loss: 3.1529 | Train Acc: 27.663% (2296/8300)\n",
      "Epoch 0 - Batch 83/500: Loss: 3.1339 | Train Acc: 27.833% (2338/8400)\n",
      "Epoch 0 - Batch 84/500: Loss: 3.1166 | Train Acc: 28.012% (2381/8500)\n",
      "Epoch 0 - Batch 85/500: Loss: 3.0991 | Train Acc: 28.140% (2420/8600)\n",
      "Epoch 0 - Batch 86/500: Loss: 3.0866 | Train Acc: 28.230% (2456/8700)\n",
      "Epoch 0 - Batch 87/500: Loss: 3.0709 | Train Acc: 28.375% (2497/8800)\n",
      "Epoch 0 - Batch 88/500: Loss: 3.0574 | Train Acc: 28.404% (2528/8900)\n",
      "Epoch 0 - Batch 89/500: Loss: 3.0451 | Train Acc: 28.456% (2561/9000)\n",
      "Epoch 0 - Batch 90/500: Loss: 3.0295 | Train Acc: 28.527% (2596/9100)\n",
      "Epoch 0 - Batch 91/500: Loss: 3.0159 | Train Acc: 28.620% (2633/9200)\n",
      "Epoch 0 - Batch 92/500: Loss: 3.0007 | Train Acc: 28.753% (2674/9300)\n",
      "Epoch 0 - Batch 93/500: Loss: 2.9869 | Train Acc: 28.936% (2720/9400)\n",
      "Epoch 0 - Batch 94/500: Loss: 2.9773 | Train Acc: 28.916% (2747/9500)\n",
      "Epoch 0 - Batch 95/500: Loss: 2.9666 | Train Acc: 29.000% (2784/9600)\n",
      "Epoch 0 - Batch 96/500: Loss: 2.9566 | Train Acc: 28.990% (2812/9700)\n",
      "Epoch 0 - Batch 97/500: Loss: 2.9449 | Train Acc: 29.092% (2851/9800)\n",
      "Epoch 0 - Batch 98/500: Loss: 2.9338 | Train Acc: 29.101% (2881/9900)\n",
      "Epoch 0 - Batch 99/500: Loss: 2.9226 | Train Acc: 29.150% (2915/10000)\n",
      "Epoch 0 - Batch 100/500: Loss: 2.9125 | Train Acc: 29.228% (2952/10100)\n",
      "Epoch 0 - Batch 101/500: Loss: 2.9018 | Train Acc: 29.333% (2992/10200)\n",
      "Epoch 0 - Batch 102/500: Loss: 2.8913 | Train Acc: 29.447% (3033/10300)\n",
      "Epoch 0 - Batch 103/500: Loss: 2.8798 | Train Acc: 29.558% (3074/10400)\n",
      "Epoch 0 - Batch 104/500: Loss: 2.8703 | Train Acc: 29.657% (3114/10500)\n",
      "Epoch 0 - Batch 105/500: Loss: 2.8605 | Train Acc: 29.679% (3146/10600)\n",
      "Epoch 0 - Batch 106/500: Loss: 2.8520 | Train Acc: 29.682% (3176/10700)\n",
      "Epoch 0 - Batch 107/500: Loss: 2.8443 | Train Acc: 29.731% (3211/10800)\n",
      "Epoch 0 - Batch 108/500: Loss: 2.8321 | Train Acc: 29.826% (3251/10900)\n",
      "Epoch 0 - Batch 109/500: Loss: 2.8218 | Train Acc: 29.809% (3279/11000)\n",
      "Epoch 0 - Batch 110/500: Loss: 2.8124 | Train Acc: 29.892% (3318/11100)\n",
      "Epoch 0 - Batch 111/500: Loss: 2.8033 | Train Acc: 30.009% (3361/11200)\n",
      "Epoch 0 - Batch 112/500: Loss: 2.7929 | Train Acc: 30.044% (3395/11300)\n",
      "Epoch 0 - Batch 113/500: Loss: 2.7844 | Train Acc: 30.132% (3435/11400)\n",
      "Epoch 0 - Batch 114/500: Loss: 2.7750 | Train Acc: 30.217% (3475/11500)\n",
      "Epoch 0 - Batch 115/500: Loss: 2.7663 | Train Acc: 30.284% (3513/11600)\n",
      "Epoch 0 - Batch 116/500: Loss: 2.7566 | Train Acc: 30.410% (3558/11700)\n",
      "Epoch 0 - Batch 117/500: Loss: 2.7473 | Train Acc: 30.449% (3593/11800)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Batch 118/500: Loss: 2.7390 | Train Acc: 30.479% (3627/11900)\n",
      "Epoch 0 - Batch 119/500: Loss: 2.7300 | Train Acc: 30.592% (3671/12000)\n",
      "Epoch 0 - Batch 120/500: Loss: 2.7219 | Train Acc: 30.653% (3709/12100)\n",
      "Epoch 0 - Batch 121/500: Loss: 2.7144 | Train Acc: 30.705% (3746/12200)\n",
      "Epoch 0 - Batch 122/500: Loss: 2.7070 | Train Acc: 30.724% (3779/12300)\n",
      "Epoch 0 - Batch 123/500: Loss: 2.6992 | Train Acc: 30.774% (3816/12400)\n",
      "Epoch 0 - Batch 124/500: Loss: 2.6915 | Train Acc: 30.856% (3857/12500)\n",
      "Epoch 0 - Batch 125/500: Loss: 2.6852 | Train Acc: 30.905% (3894/12600)\n",
      "Epoch 0 - Batch 126/500: Loss: 2.6772 | Train Acc: 30.984% (3935/12700)\n",
      "Epoch 0 - Batch 127/500: Loss: 2.6690 | Train Acc: 31.156% (3988/12800)\n",
      "Epoch 0 - Batch 128/500: Loss: 2.6604 | Train Acc: 31.279% (4035/12900)\n",
      "Epoch 0 - Batch 129/500: Loss: 2.6540 | Train Acc: 31.331% (4073/13000)\n",
      "Epoch 0 - Batch 130/500: Loss: 2.6464 | Train Acc: 31.412% (4115/13100)\n",
      "Epoch 0 - Batch 131/500: Loss: 2.6377 | Train Acc: 31.492% (4157/13200)\n",
      "Epoch 0 - Batch 132/500: Loss: 2.6300 | Train Acc: 31.571% (4199/13300)\n",
      "Epoch 0 - Batch 133/500: Loss: 2.6222 | Train Acc: 31.619% (4237/13400)\n",
      "Epoch 0 - Batch 134/500: Loss: 2.6156 | Train Acc: 31.689% (4278/13500)\n",
      "Epoch 0 - Batch 135/500: Loss: 2.6093 | Train Acc: 31.721% (4314/13600)\n",
      "Epoch 0 - Batch 136/500: Loss: 2.6014 | Train Acc: 31.803% (4357/13700)\n",
      "Epoch 0 - Batch 137/500: Loss: 2.5936 | Train Acc: 31.870% (4398/13800)\n",
      "Epoch 0 - Batch 138/500: Loss: 2.5885 | Train Acc: 31.835% (4425/13900)\n",
      "Epoch 0 - Batch 139/500: Loss: 2.5819 | Train Acc: 31.879% (4463/14000)\n",
      "Epoch 0 - Batch 140/500: Loss: 2.5749 | Train Acc: 31.965% (4507/14100)\n",
      "Epoch 0 - Batch 141/500: Loss: 2.5687 | Train Acc: 31.993% (4543/14200)\n",
      "Epoch 0 - Batch 142/500: Loss: 2.5627 | Train Acc: 32.021% (4579/14300)\n",
      "Epoch 0 - Batch 143/500: Loss: 2.5570 | Train Acc: 32.069% (4618/14400)\n",
      "Epoch 0 - Batch 144/500: Loss: 2.5515 | Train Acc: 32.117% (4657/14500)\n",
      "Epoch 0 - Batch 145/500: Loss: 2.5450 | Train Acc: 32.185% (4699/14600)\n",
      "Epoch 0 - Batch 146/500: Loss: 2.5381 | Train Acc: 32.252% (4741/14700)\n",
      "Epoch 0 - Batch 147/500: Loss: 2.5327 | Train Acc: 32.291% (4779/14800)\n",
      "Epoch 0 - Batch 148/500: Loss: 2.5263 | Train Acc: 32.389% (4826/14900)\n",
      "Epoch 0 - Batch 149/500: Loss: 2.5207 | Train Acc: 32.493% (4874/15000)\n",
      "Epoch 0 - Batch 150/500: Loss: 2.5152 | Train Acc: 32.517% (4910/15100)\n",
      "Epoch 0 - Batch 151/500: Loss: 2.5089 | Train Acc: 32.553% (4948/15200)\n",
      "Epoch 0 - Batch 152/500: Loss: 2.5027 | Train Acc: 32.627% (4992/15300)\n",
      "Epoch 0 - Batch 153/500: Loss: 2.4974 | Train Acc: 32.688% (5034/15400)\n",
      "Epoch 0 - Batch 154/500: Loss: 2.4909 | Train Acc: 32.806% (5085/15500)\n",
      "Epoch 0 - Batch 155/500: Loss: 2.4852 | Train Acc: 32.885% (5130/15600)\n",
      "Epoch 0 - Batch 156/500: Loss: 2.4806 | Train Acc: 32.892% (5164/15700)\n",
      "Epoch 0 - Batch 157/500: Loss: 2.4747 | Train Acc: 32.956% (5207/15800)\n",
      "Epoch 0 - Batch 158/500: Loss: 2.4681 | Train Acc: 33.050% (5255/15900)\n",
      "Epoch 0 - Batch 159/500: Loss: 2.4619 | Train Acc: 33.125% (5300/16000)\n",
      "Epoch 0 - Batch 160/500: Loss: 2.4563 | Train Acc: 33.161% (5339/16100)\n",
      "Epoch 0 - Batch 161/500: Loss: 2.4497 | Train Acc: 33.290% (5393/16200)\n",
      "Epoch 0 - Batch 162/500: Loss: 2.4449 | Train Acc: 33.337% (5434/16300)\n",
      "Epoch 0 - Batch 163/500: Loss: 2.4389 | Train Acc: 33.396% (5477/16400)\n",
      "Epoch 0 - Batch 164/500: Loss: 2.4326 | Train Acc: 33.473% (5523/16500)\n",
      "Epoch 0 - Batch 165/500: Loss: 2.4265 | Train Acc: 33.536% (5567/16600)\n",
      "Epoch 0 - Batch 166/500: Loss: 2.4204 | Train Acc: 33.599% (5611/16700)\n",
      "Epoch 0 - Batch 167/500: Loss: 2.4148 | Train Acc: 33.690% (5660/16800)\n",
      "Epoch 0 - Batch 168/500: Loss: 2.4103 | Train Acc: 33.722% (5699/16900)\n",
      "Epoch 0 - Batch 169/500: Loss: 2.4047 | Train Acc: 33.806% (5747/17000)\n",
      "Epoch 0 - Batch 170/500: Loss: 2.3995 | Train Acc: 33.860% (5790/17100)\n",
      "Epoch 0 - Batch 171/500: Loss: 2.3962 | Train Acc: 33.872% (5826/17200)\n",
      "Epoch 0 - Batch 172/500: Loss: 2.3907 | Train Acc: 33.925% (5869/17300)\n",
      "Epoch 0 - Batch 173/500: Loss: 2.3863 | Train Acc: 33.989% (5914/17400)\n",
      "Epoch 0 - Batch 174/500: Loss: 2.3824 | Train Acc: 34.011% (5952/17500)\n",
      "Epoch 0 - Batch 175/500: Loss: 2.3775 | Train Acc: 34.062% (5995/17600)\n",
      "Epoch 0 - Batch 176/500: Loss: 2.3722 | Train Acc: 34.136% (6042/17700)\n",
      "Epoch 0 - Batch 177/500: Loss: 2.3662 | Train Acc: 34.225% (6092/17800)\n",
      "Epoch 0 - Batch 178/500: Loss: 2.3622 | Train Acc: 34.268% (6134/17900)\n",
      "Epoch 0 - Batch 179/500: Loss: 2.3581 | Train Acc: 34.350% (6183/18000)\n",
      "Epoch 0 - Batch 180/500: Loss: 2.3528 | Train Acc: 34.403% (6227/18100)\n",
      "Epoch 0 - Batch 181/500: Loss: 2.3478 | Train Acc: 34.489% (6277/18200)\n",
      "Epoch 0 - Batch 182/500: Loss: 2.3438 | Train Acc: 34.568% (6326/18300)\n",
      "Epoch 0 - Batch 183/500: Loss: 2.3385 | Train Acc: 34.690% (6383/18400)\n",
      "Epoch 0 - Batch 184/500: Loss: 2.3342 | Train Acc: 34.735% (6426/18500)\n",
      "Epoch 0 - Batch 185/500: Loss: 2.3300 | Train Acc: 34.801% (6473/18600)\n",
      "Epoch 0 - Batch 186/500: Loss: 2.3248 | Train Acc: 34.877% (6522/18700)\n",
      "Epoch 0 - Batch 187/500: Loss: 2.3209 | Train Acc: 34.910% (6563/18800)\n",
      "Epoch 0 - Batch 188/500: Loss: 2.3173 | Train Acc: 34.974% (6610/18900)\n",
      "Epoch 0 - Batch 189/500: Loss: 2.3125 | Train Acc: 35.037% (6657/19000)\n",
      "Epoch 0 - Batch 190/500: Loss: 2.3095 | Train Acc: 35.016% (6688/19100)\n",
      "Epoch 0 - Batch 191/500: Loss: 2.3042 | Train Acc: 35.099% (6739/19200)\n",
      "Epoch 0 - Batch 192/500: Loss: 2.3006 | Train Acc: 35.119% (6778/19300)\n",
      "Epoch 0 - Batch 193/500: Loss: 2.2963 | Train Acc: 35.186% (6826/19400)\n",
      "Epoch 0 - Batch 194/500: Loss: 2.2926 | Train Acc: 35.221% (6868/19500)\n",
      "Epoch 0 - Batch 195/500: Loss: 2.2881 | Train Acc: 35.301% (6919/19600)\n",
      "Epoch 0 - Batch 196/500: Loss: 2.2831 | Train Acc: 35.360% (6966/19700)\n",
      "Epoch 0 - Batch 197/500: Loss: 2.2791 | Train Acc: 35.399% (7009/19800)\n",
      "Epoch 0 - Batch 198/500: Loss: 2.2752 | Train Acc: 35.447% (7054/19900)\n",
      "Epoch 0 - Batch 199/500: Loss: 2.2705 | Train Acc: 35.500% (7100/20000)\n",
      "Epoch 0 - Batch 200/500: Loss: 2.2660 | Train Acc: 35.572% (7150/20100)\n",
      "Epoch 0 - Batch 201/500: Loss: 2.2621 | Train Acc: 35.639% (7199/20200)\n",
      "Epoch 0 - Batch 202/500: Loss: 2.2583 | Train Acc: 35.685% (7244/20300)\n",
      "Epoch 0 - Batch 203/500: Loss: 2.2554 | Train Acc: 35.686% (7280/20400)\n",
      "Epoch 0 - Batch 204/500: Loss: 2.2510 | Train Acc: 35.785% (7336/20500)\n",
      "Epoch 0 - Batch 205/500: Loss: 2.2474 | Train Acc: 35.835% (7382/20600)\n",
      "Epoch 0 - Batch 206/500: Loss: 2.2461 | Train Acc: 35.821% (7415/20700)\n",
      "Epoch 0 - Batch 207/500: Loss: 2.2431 | Train Acc: 35.832% (7453/20800)\n",
      "Epoch 0 - Batch 208/500: Loss: 2.2390 | Train Acc: 35.914% (7506/20900)\n",
      "Epoch 0 - Batch 209/500: Loss: 2.2356 | Train Acc: 35.971% (7554/21000)\n",
      "Epoch 0 - Batch 210/500: Loss: 2.2328 | Train Acc: 35.995% (7595/21100)\n",
      "Epoch 0 - Batch 211/500: Loss: 2.2295 | Train Acc: 36.019% (7636/21200)\n",
      "Epoch 0 - Batch 212/500: Loss: 2.2266 | Train Acc: 36.052% (7679/21300)\n",
      "Epoch 0 - Batch 213/500: Loss: 2.2234 | Train Acc: 36.051% (7715/21400)\n",
      "Epoch 0 - Batch 214/500: Loss: 2.2205 | Train Acc: 36.084% (7758/21500)\n",
      "Epoch 0 - Batch 215/500: Loss: 2.2159 | Train Acc: 36.190% (7817/21600)\n",
      "Epoch 0 - Batch 216/500: Loss: 2.2124 | Train Acc: 36.240% (7864/21700)\n",
      "Epoch 0 - Batch 217/500: Loss: 2.2086 | Train Acc: 36.289% (7911/21800)\n",
      "Epoch 0 - Batch 218/500: Loss: 2.2057 | Train Acc: 36.292% (7948/21900)\n",
      "Epoch 0 - Batch 219/500: Loss: 2.2029 | Train Acc: 36.341% (7995/22000)\n",
      "Epoch 0 - Batch 220/500: Loss: 2.1996 | Train Acc: 36.394% (8043/22100)\n",
      "Epoch 0 - Batch 221/500: Loss: 2.1965 | Train Acc: 36.459% (8094/22200)\n",
      "Epoch 0 - Batch 222/500: Loss: 2.1928 | Train Acc: 36.543% (8149/22300)\n",
      "Epoch 0 - Batch 223/500: Loss: 2.1893 | Train Acc: 36.612% (8201/22400)\n",
      "Epoch 0 - Batch 224/500: Loss: 2.1856 | Train Acc: 36.702% (8258/22500)\n",
      "Epoch 0 - Batch 225/500: Loss: 2.1822 | Train Acc: 36.770% (8310/22600)\n",
      "Epoch 0 - Batch 226/500: Loss: 2.1790 | Train Acc: 36.819% (8358/22700)\n",
      "Epoch 0 - Batch 227/500: Loss: 2.1756 | Train Acc: 36.868% (8406/22800)\n",
      "Epoch 0 - Batch 228/500: Loss: 2.1720 | Train Acc: 36.921% (8455/22900)\n",
      "Epoch 0 - Batch 229/500: Loss: 2.1682 | Train Acc: 36.996% (8509/23000)\n",
      "Epoch 0 - Batch 230/500: Loss: 2.1654 | Train Acc: 37.026% (8553/23100)\n",
      "Epoch 0 - Batch 231/500: Loss: 2.1622 | Train Acc: 37.065% (8599/23200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Batch 232/500: Loss: 2.1598 | Train Acc: 37.107% (8646/23300)\n",
      "Epoch 0 - Batch 233/500: Loss: 2.1574 | Train Acc: 37.167% (8697/23400)\n",
      "Epoch 0 - Batch 234/500: Loss: 2.1537 | Train Acc: 37.221% (8747/23500)\n",
      "Epoch 0 - Batch 235/500: Loss: 2.1503 | Train Acc: 37.280% (8798/23600)\n",
      "Epoch 0 - Batch 236/500: Loss: 2.1476 | Train Acc: 37.316% (8844/23700)\n",
      "Epoch 0 - Batch 237/500: Loss: 2.1450 | Train Acc: 37.378% (8896/23800)\n",
      "Epoch 0 - Batch 238/500: Loss: 2.1428 | Train Acc: 37.389% (8936/23900)\n",
      "Epoch 0 - Batch 239/500: Loss: 2.1398 | Train Acc: 37.438% (8985/24000)\n",
      "Epoch 0 - Batch 240/500: Loss: 2.1368 | Train Acc: 37.527% (9044/24100)\n",
      "Epoch 0 - Batch 241/500: Loss: 2.1350 | Train Acc: 37.554% (9088/24200)\n",
      "Epoch 0 - Batch 242/500: Loss: 2.1315 | Train Acc: 37.634% (9145/24300)\n",
      "Epoch 0 - Batch 243/500: Loss: 2.1286 | Train Acc: 37.680% (9194/24400)\n",
      "Epoch 0 - Batch 244/500: Loss: 2.1264 | Train Acc: 37.690% (9234/24500)\n",
      "Epoch 0 - Batch 245/500: Loss: 2.1234 | Train Acc: 37.732% (9282/24600)\n",
      "Epoch 0 - Batch 246/500: Loss: 2.1206 | Train Acc: 37.773% (9330/24700)\n",
      "Epoch 0 - Batch 247/500: Loss: 2.1174 | Train Acc: 37.843% (9385/24800)\n",
      "Epoch 0 - Batch 248/500: Loss: 2.1151 | Train Acc: 37.855% (9426/24900)\n",
      "Epoch 0 - Batch 249/500: Loss: 2.1126 | Train Acc: 37.888% (9472/25000)\n",
      "Epoch 0 - Batch 250/500: Loss: 2.1100 | Train Acc: 37.944% (9524/25100)\n",
      "Epoch 0 - Batch 251/500: Loss: 2.1064 | Train Acc: 38.028% (9583/25200)\n",
      "Epoch 0 - Batch 252/500: Loss: 2.1030 | Train Acc: 38.111% (9642/25300)\n",
      "Epoch 0 - Batch 253/500: Loss: 2.1001 | Train Acc: 38.150% (9690/25400)\n",
      "Epoch 0 - Batch 254/500: Loss: 2.0968 | Train Acc: 38.216% (9745/25500)\n",
      "Epoch 0 - Batch 255/500: Loss: 2.0936 | Train Acc: 38.273% (9798/25600)\n",
      "Epoch 0 - Batch 256/500: Loss: 2.0921 | Train Acc: 38.288% (9840/25700)\n",
      "Epoch 0 - Batch 257/500: Loss: 2.0894 | Train Acc: 38.302% (9882/25800)\n",
      "Epoch 0 - Batch 258/500: Loss: 2.0871 | Train Acc: 38.320% (9925/25900)\n",
      "Epoch 0 - Batch 259/500: Loss: 2.0846 | Train Acc: 38.335% (9967/26000)\n",
      "Epoch 0 - Batch 260/500: Loss: 2.0824 | Train Acc: 38.368% (10014/26100)\n",
      "Epoch 0 - Batch 261/500: Loss: 2.0797 | Train Acc: 38.412% (10064/26200)\n",
      "Epoch 0 - Batch 262/500: Loss: 2.0771 | Train Acc: 38.441% (10110/26300)\n",
      "Epoch 0 - Batch 263/500: Loss: 2.0754 | Train Acc: 38.455% (10152/26400)\n",
      "Epoch 0 - Batch 264/500: Loss: 2.0735 | Train Acc: 38.460% (10192/26500)\n",
      "Epoch 0 - Batch 265/500: Loss: 2.0710 | Train Acc: 38.526% (10248/26600)\n",
      "Epoch 0 - Batch 266/500: Loss: 2.0688 | Train Acc: 38.551% (10293/26700)\n",
      "Epoch 0 - Batch 267/500: Loss: 2.0670 | Train Acc: 38.556% (10333/26800)\n",
      "Epoch 0 - Batch 268/500: Loss: 2.0647 | Train Acc: 38.591% (10381/26900)\n",
      "Epoch 0 - Batch 269/500: Loss: 2.0628 | Train Acc: 38.615% (10426/27000)\n",
      "Epoch 0 - Batch 270/500: Loss: 2.0602 | Train Acc: 38.661% (10477/27100)\n",
      "Epoch 0 - Batch 271/500: Loss: 2.0573 | Train Acc: 38.721% (10532/27200)\n",
      "Epoch 0 - Batch 272/500: Loss: 2.0552 | Train Acc: 38.747% (10578/27300)\n",
      "Epoch 0 - Batch 273/500: Loss: 2.0530 | Train Acc: 38.777% (10625/27400)\n",
      "Epoch 0 - Batch 274/500: Loss: 2.0506 | Train Acc: 38.800% (10670/27500)\n",
      "Epoch 0 - Batch 275/500: Loss: 2.0485 | Train Acc: 38.808% (10711/27600)\n",
      "Epoch 0 - Batch 276/500: Loss: 2.0464 | Train Acc: 38.838% (10758/27700)\n",
      "Epoch 0 - Batch 277/500: Loss: 2.0450 | Train Acc: 38.824% (10793/27800)\n",
      "Epoch 0 - Batch 278/500: Loss: 2.0431 | Train Acc: 38.846% (10838/27900)\n",
      "Epoch 0 - Batch 279/500: Loss: 2.0407 | Train Acc: 38.896% (10891/28000)\n",
      "Epoch 0 - Batch 280/500: Loss: 2.0382 | Train Acc: 38.964% (10949/28100)\n",
      "Epoch 0 - Batch 281/500: Loss: 2.0352 | Train Acc: 39.032% (11007/28200)\n",
      "Epoch 0 - Batch 282/500: Loss: 2.0327 | Train Acc: 39.081% (11060/28300)\n",
      "Epoch 0 - Batch 283/500: Loss: 2.0307 | Train Acc: 39.137% (11115/28400)\n",
      "Epoch 0 - Batch 284/500: Loss: 2.0286 | Train Acc: 39.151% (11158/28500)\n",
      "Epoch 0 - Batch 285/500: Loss: 2.0262 | Train Acc: 39.199% (11211/28600)\n",
      "Epoch 0 - Batch 286/500: Loss: 2.0238 | Train Acc: 39.251% (11265/28700)\n",
      "Epoch 0 - Batch 287/500: Loss: 2.0222 | Train Acc: 39.274% (11311/28800)\n",
      "Epoch 0 - Batch 288/500: Loss: 2.0196 | Train Acc: 39.311% (11361/28900)\n",
      "Epoch 0 - Batch 289/500: Loss: 2.0182 | Train Acc: 39.310% (11400/29000)\n",
      "Epoch 0 - Batch 290/500: Loss: 2.0163 | Train Acc: 39.344% (11449/29100)\n",
      "Epoch 0 - Batch 291/500: Loss: 2.0137 | Train Acc: 39.384% (11500/29200)\n",
      "Epoch 0 - Batch 292/500: Loss: 2.0119 | Train Acc: 39.423% (11551/29300)\n",
      "Epoch 0 - Batch 293/500: Loss: 2.0100 | Train Acc: 39.456% (11600/29400)\n",
      "Epoch 0 - Batch 294/500: Loss: 2.0076 | Train Acc: 39.512% (11656/29500)\n",
      "Epoch 0 - Batch 295/500: Loss: 2.0057 | Train Acc: 39.547% (11706/29600)\n",
      "Epoch 0 - Batch 296/500: Loss: 2.0036 | Train Acc: 39.589% (11758/29700)\n",
      "Epoch 0 - Batch 297/500: Loss: 2.0014 | Train Acc: 39.624% (11808/29800)\n",
      "Epoch 0 - Batch 298/500: Loss: 1.9994 | Train Acc: 39.649% (11855/29900)\n",
      "Epoch 0 - Batch 299/500: Loss: 1.9976 | Train Acc: 39.650% (11895/30000)\n",
      "Epoch 0 - Batch 300/500: Loss: 1.9959 | Train Acc: 39.674% (11942/30100)\n",
      "Epoch 0 - Batch 301/500: Loss: 1.9933 | Train Acc: 39.709% (11992/30200)\n",
      "Epoch 0 - Batch 302/500: Loss: 1.9919 | Train Acc: 39.706% (12031/30300)\n",
      "Epoch 0 - Batch 303/500: Loss: 1.9896 | Train Acc: 39.753% (12085/30400)\n",
      "Epoch 0 - Batch 304/500: Loss: 1.9887 | Train Acc: 39.748% (12123/30500)\n",
      "Epoch 0 - Batch 305/500: Loss: 1.9873 | Train Acc: 39.775% (12171/30600)\n",
      "Epoch 0 - Batch 306/500: Loss: 1.9849 | Train Acc: 39.808% (12221/30700)\n",
      "Epoch 0 - Batch 307/500: Loss: 1.9824 | Train Acc: 39.877% (12282/30800)\n",
      "Epoch 0 - Batch 308/500: Loss: 1.9801 | Train Acc: 39.919% (12335/30900)\n",
      "Epoch 0 - Batch 309/500: Loss: 1.9782 | Train Acc: 39.961% (12388/31000)\n",
      "Epoch 0 - Batch 310/500: Loss: 1.9765 | Train Acc: 39.981% (12434/31100)\n",
      "Epoch 0 - Batch 311/500: Loss: 1.9750 | Train Acc: 39.994% (12478/31200)\n",
      "Epoch 0 - Batch 312/500: Loss: 1.9729 | Train Acc: 40.038% (12532/31300)\n",
      "Epoch 0 - Batch 313/500: Loss: 1.9713 | Train Acc: 40.038% (12572/31400)\n",
      "Epoch 0 - Batch 314/500: Loss: 1.9693 | Train Acc: 40.060% (12619/31500)\n",
      "Epoch 0 - Batch 315/500: Loss: 1.9678 | Train Acc: 40.060% (12659/31600)\n",
      "Epoch 0 - Batch 316/500: Loss: 1.9664 | Train Acc: 40.073% (12703/31700)\n",
      "Epoch 0 - Batch 317/500: Loss: 1.9646 | Train Acc: 40.101% (12752/31800)\n",
      "Epoch 0 - Batch 318/500: Loss: 1.9629 | Train Acc: 40.144% (12806/31900)\n",
      "Epoch 0 - Batch 319/500: Loss: 1.9609 | Train Acc: 40.191% (12861/32000)\n",
      "Epoch 0 - Batch 320/500: Loss: 1.9601 | Train Acc: 40.199% (12904/32100)\n",
      "Epoch 0 - Batch 321/500: Loss: 1.9590 | Train Acc: 40.202% (12945/32200)\n",
      "Epoch 0 - Batch 322/500: Loss: 1.9575 | Train Acc: 40.217% (12990/32300)\n",
      "Epoch 0 - Batch 323/500: Loss: 1.9557 | Train Acc: 40.244% (13039/32400)\n",
      "Epoch 0 - Batch 324/500: Loss: 1.9538 | Train Acc: 40.265% (13086/32500)\n",
      "Epoch 0 - Batch 325/500: Loss: 1.9527 | Train Acc: 40.276% (13130/32600)\n",
      "Epoch 0 - Batch 326/500: Loss: 1.9510 | Train Acc: 40.318% (13184/32700)\n",
      "Epoch 0 - Batch 327/500: Loss: 1.9496 | Train Acc: 40.332% (13229/32800)\n",
      "Epoch 0 - Batch 328/500: Loss: 1.9480 | Train Acc: 40.340% (13272/32900)\n",
      "Epoch 0 - Batch 329/500: Loss: 1.9466 | Train Acc: 40.352% (13316/33000)\n",
      "Epoch 0 - Batch 330/500: Loss: 1.9449 | Train Acc: 40.393% (13370/33100)\n",
      "Epoch 0 - Batch 331/500: Loss: 1.9436 | Train Acc: 40.416% (13418/33200)\n",
      "Epoch 0 - Batch 332/500: Loss: 1.9421 | Train Acc: 40.423% (13461/33300)\n",
      "Epoch 0 - Batch 333/500: Loss: 1.9405 | Train Acc: 40.455% (13512/33400)\n",
      "Epoch 0 - Batch 334/500: Loss: 1.9384 | Train Acc: 40.501% (13568/33500)\n",
      "Epoch 0 - Batch 335/500: Loss: 1.9373 | Train Acc: 40.509% (13611/33600)\n",
      "Epoch 0 - Batch 336/500: Loss: 1.9361 | Train Acc: 40.528% (13658/33700)\n",
      "Epoch 0 - Batch 337/500: Loss: 1.9346 | Train Acc: 40.547% (13705/33800)\n",
      "Epoch 0 - Batch 338/500: Loss: 1.9329 | Train Acc: 40.575% (13755/33900)\n",
      "Epoch 0 - Batch 339/500: Loss: 1.9318 | Train Acc: 40.594% (13802/34000)\n",
      "Epoch 0 - Batch 340/500: Loss: 1.9301 | Train Acc: 40.630% (13855/34100)\n",
      "Epoch 0 - Batch 341/500: Loss: 1.9287 | Train Acc: 40.652% (13903/34200)\n",
      "Epoch 0 - Batch 342/500: Loss: 1.9269 | Train Acc: 40.700% (13960/34300)\n",
      "Epoch 0 - Batch 343/500: Loss: 1.9255 | Train Acc: 40.709% (14004/34400)\n",
      "Epoch 0 - Batch 344/500: Loss: 1.9237 | Train Acc: 40.733% (14053/34500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Batch 345/500: Loss: 1.9216 | Train Acc: 40.772% (14107/34600)\n",
      "Epoch 0 - Batch 346/500: Loss: 1.9202 | Train Acc: 40.810% (14161/34700)\n",
      "Epoch 0 - Batch 347/500: Loss: 1.9190 | Train Acc: 40.851% (14216/34800)\n",
      "Epoch 0 - Batch 348/500: Loss: 1.9175 | Train Acc: 40.865% (14262/34900)\n",
      "Epoch 0 - Batch 349/500: Loss: 1.9162 | Train Acc: 40.880% (14308/35000)\n",
      "Epoch 0 - Batch 350/500: Loss: 1.9148 | Train Acc: 40.917% (14362/35100)\n",
      "Epoch 0 - Batch 351/500: Loss: 1.9132 | Train Acc: 40.932% (14408/35200)\n",
      "Epoch 0 - Batch 352/500: Loss: 1.9126 | Train Acc: 40.946% (14454/35300)\n",
      "Epoch 0 - Batch 353/500: Loss: 1.9112 | Train Acc: 40.969% (14503/35400)\n",
      "Epoch 0 - Batch 354/500: Loss: 1.9094 | Train Acc: 40.975% (14546/35500)\n",
      "Epoch 0 - Batch 355/500: Loss: 1.9080 | Train Acc: 40.997% (14595/35600)\n",
      "Epoch 0 - Batch 356/500: Loss: 1.9063 | Train Acc: 41.031% (14648/35700)\n",
      "Epoch 0 - Batch 357/500: Loss: 1.9052 | Train Acc: 41.045% (14694/35800)\n",
      "Epoch 0 - Batch 358/500: Loss: 1.9038 | Train Acc: 41.061% (14741/35900)\n",
      "Epoch 0 - Batch 359/500: Loss: 1.9020 | Train Acc: 41.083% (14790/36000)\n",
      "Epoch 0 - Batch 360/500: Loss: 1.9010 | Train Acc: 41.075% (14828/36100)\n",
      "Epoch 0 - Batch 361/500: Loss: 1.8997 | Train Acc: 41.091% (14875/36200)\n",
      "Epoch 0 - Batch 362/500: Loss: 1.8982 | Train Acc: 41.135% (14932/36300)\n",
      "Epoch 0 - Batch 363/500: Loss: 1.8968 | Train Acc: 41.170% (14986/36400)\n",
      "Epoch 0 - Batch 364/500: Loss: 1.8951 | Train Acc: 41.205% (15040/36500)\n",
      "Epoch 0 - Batch 365/500: Loss: 1.8936 | Train Acc: 41.235% (15092/36600)\n",
      "Epoch 0 - Batch 366/500: Loss: 1.8917 | Train Acc: 41.278% (15149/36700)\n",
      "Epoch 0 - Batch 367/500: Loss: 1.8906 | Train Acc: 41.291% (15195/36800)\n",
      "Epoch 0 - Batch 368/500: Loss: 1.8895 | Train Acc: 41.290% (15236/36900)\n",
      "Epoch 0 - Batch 369/500: Loss: 1.8881 | Train Acc: 41.319% (15288/37000)\n",
      "Epoch 0 - Batch 370/500: Loss: 1.8867 | Train Acc: 41.329% (15333/37100)\n",
      "Epoch 0 - Batch 371/500: Loss: 1.8851 | Train Acc: 41.344% (15380/37200)\n",
      "Epoch 0 - Batch 372/500: Loss: 1.8836 | Train Acc: 41.375% (15433/37300)\n",
      "Epoch 0 - Batch 373/500: Loss: 1.8822 | Train Acc: 41.409% (15487/37400)\n",
      "Epoch 0 - Batch 374/500: Loss: 1.8804 | Train Acc: 41.469% (15551/37500)\n",
      "Epoch 0 - Batch 375/500: Loss: 1.8788 | Train Acc: 41.503% (15605/37600)\n",
      "Epoch 0 - Batch 376/500: Loss: 1.8771 | Train Acc: 41.520% (15653/37700)\n",
      "Epoch 0 - Batch 377/500: Loss: 1.8755 | Train Acc: 41.558% (15709/37800)\n",
      "Epoch 0 - Batch 378/500: Loss: 1.8747 | Train Acc: 41.586% (15761/37900)\n",
      "Epoch 0 - Batch 379/500: Loss: 1.8735 | Train Acc: 41.611% (15812/38000)\n",
      "Epoch 0 - Batch 380/500: Loss: 1.8722 | Train Acc: 41.648% (15868/38100)\n",
      "Epoch 0 - Batch 381/500: Loss: 1.8709 | Train Acc: 41.686% (15924/38200)\n",
      "Epoch 0 - Batch 382/500: Loss: 1.8693 | Train Acc: 41.718% (15978/38300)\n",
      "Epoch 0 - Batch 383/500: Loss: 1.8675 | Train Acc: 41.755% (16034/38400)\n",
      "Epoch 0 - Batch 384/500: Loss: 1.8664 | Train Acc: 41.761% (16078/38500)\n",
      "Epoch 0 - Batch 385/500: Loss: 1.8647 | Train Acc: 41.790% (16131/38600)\n",
      "Epoch 0 - Batch 386/500: Loss: 1.8634 | Train Acc: 41.804% (16178/38700)\n",
      "Epoch 0 - Batch 387/500: Loss: 1.8621 | Train Acc: 41.820% (16226/38800)\n",
      "Epoch 0 - Batch 388/500: Loss: 1.8613 | Train Acc: 41.833% (16273/38900)\n",
      "Epoch 0 - Batch 389/500: Loss: 1.8598 | Train Acc: 41.869% (16329/39000)\n",
      "Epoch 0 - Batch 390/500: Loss: 1.8588 | Train Acc: 41.887% (16378/39100)\n",
      "Epoch 0 - Batch 391/500: Loss: 1.8574 | Train Acc: 41.911% (16429/39200)\n",
      "Epoch 0 - Batch 392/500: Loss: 1.8559 | Train Acc: 41.926% (16477/39300)\n",
      "Epoch 0 - Batch 393/500: Loss: 1.8544 | Train Acc: 41.957% (16531/39400)\n",
      "Epoch 0 - Batch 394/500: Loss: 1.8528 | Train Acc: 41.985% (16584/39500)\n",
      "Epoch 0 - Batch 395/500: Loss: 1.8517 | Train Acc: 42.003% (16633/39600)\n",
      "Epoch 0 - Batch 396/500: Loss: 1.8505 | Train Acc: 42.020% (16682/39700)\n",
      "Epoch 0 - Batch 397/500: Loss: 1.8488 | Train Acc: 42.070% (16744/39800)\n",
      "Epoch 0 - Batch 398/500: Loss: 1.8473 | Train Acc: 42.108% (16801/39900)\n",
      "Epoch 0 - Batch 399/500: Loss: 1.8458 | Train Acc: 42.135% (16854/40000)\n",
      "Epoch 0 - Batch 400/500: Loss: 1.8446 | Train Acc: 42.150% (16902/40100)\n",
      "Epoch 0 - Batch 401/500: Loss: 1.8438 | Train Acc: 42.152% (16945/40200)\n",
      "Epoch 0 - Batch 402/500: Loss: 1.8429 | Train Acc: 42.164% (16992/40300)\n",
      "Epoch 0 - Batch 403/500: Loss: 1.8415 | Train Acc: 42.198% (17048/40400)\n",
      "Epoch 0 - Batch 404/500: Loss: 1.8404 | Train Acc: 42.217% (17098/40500)\n",
      "Epoch 0 - Batch 405/500: Loss: 1.8389 | Train Acc: 42.249% (17153/40600)\n",
      "Epoch 0 - Batch 406/500: Loss: 1.8372 | Train Acc: 42.295% (17214/40700)\n",
      "Epoch 0 - Batch 407/500: Loss: 1.8357 | Train Acc: 42.316% (17265/40800)\n",
      "Epoch 0 - Batch 408/500: Loss: 1.8341 | Train Acc: 42.357% (17324/40900)\n",
      "Epoch 0 - Batch 409/500: Loss: 1.8330 | Train Acc: 42.398% (17383/41000)\n",
      "Epoch 0 - Batch 410/500: Loss: 1.8319 | Train Acc: 42.414% (17432/41100)\n",
      "Epoch 0 - Batch 411/500: Loss: 1.8306 | Train Acc: 42.454% (17491/41200)\n",
      "Epoch 0 - Batch 412/500: Loss: 1.8293 | Train Acc: 42.489% (17548/41300)\n",
      "Epoch 0 - Batch 413/500: Loss: 1.8284 | Train Acc: 42.500% (17595/41400)\n",
      "Epoch 0 - Batch 414/500: Loss: 1.8273 | Train Acc: 42.506% (17640/41500)\n",
      "Epoch 0 - Batch 415/500: Loss: 1.8262 | Train Acc: 42.519% (17688/41600)\n",
      "Epoch 0 - Batch 416/500: Loss: 1.8255 | Train Acc: 42.535% (17737/41700)\n",
      "Epoch 0 - Batch 417/500: Loss: 1.8235 | Train Acc: 42.581% (17799/41800)\n",
      "Epoch 0 - Batch 418/500: Loss: 1.8223 | Train Acc: 42.604% (17851/41900)\n",
      "Epoch 0 - Batch 419/500: Loss: 1.8212 | Train Acc: 42.612% (17897/42000)\n",
      "Epoch 0 - Batch 420/500: Loss: 1.8198 | Train Acc: 42.637% (17950/42100)\n",
      "Epoch 0 - Batch 421/500: Loss: 1.8190 | Train Acc: 42.633% (17991/42200)\n",
      "Epoch 0 - Batch 422/500: Loss: 1.8174 | Train Acc: 42.667% (18048/42300)\n",
      "Epoch 0 - Batch 423/500: Loss: 1.8157 | Train Acc: 42.717% (18112/42400)\n",
      "Epoch 0 - Batch 424/500: Loss: 1.8148 | Train Acc: 42.722% (18157/42500)\n",
      "Epoch 0 - Batch 425/500: Loss: 1.8137 | Train Acc: 42.737% (18206/42600)\n",
      "Epoch 0 - Batch 426/500: Loss: 1.8126 | Train Acc: 42.763% (18260/42700)\n",
      "Epoch 0 - Batch 427/500: Loss: 1.8112 | Train Acc: 42.799% (18318/42800)\n",
      "Epoch 0 - Batch 428/500: Loss: 1.8101 | Train Acc: 42.823% (18371/42900)\n",
      "Epoch 0 - Batch 429/500: Loss: 1.8094 | Train Acc: 42.837% (18420/43000)\n",
      "Epoch 0 - Batch 430/500: Loss: 1.8084 | Train Acc: 42.845% (18466/43100)\n",
      "Epoch 0 - Batch 431/500: Loss: 1.8076 | Train Acc: 42.866% (18518/43200)\n",
      "Epoch 0 - Batch 432/500: Loss: 1.8067 | Train Acc: 42.875% (18565/43300)\n",
      "Epoch 0 - Batch 433/500: Loss: 1.8054 | Train Acc: 42.910% (18623/43400)\n",
      "Epoch 0 - Batch 434/500: Loss: 1.8041 | Train Acc: 42.954% (18685/43500)\n",
      "Epoch 0 - Batch 435/500: Loss: 1.8029 | Train Acc: 42.989% (18743/43600)\n",
      "Epoch 0 - Batch 436/500: Loss: 1.8017 | Train Acc: 43.018% (18799/43700)\n",
      "Epoch 0 - Batch 437/500: Loss: 1.8007 | Train Acc: 43.032% (18848/43800)\n",
      "Epoch 0 - Batch 438/500: Loss: 1.7999 | Train Acc: 43.046% (18897/43900)\n",
      "Epoch 0 - Batch 439/500: Loss: 1.7989 | Train Acc: 43.055% (18944/44000)\n",
      "Epoch 0 - Batch 440/500: Loss: 1.7977 | Train Acc: 43.077% (18997/44100)\n",
      "Epoch 0 - Batch 441/500: Loss: 1.7968 | Train Acc: 43.102% (19051/44200)\n",
      "Epoch 0 - Batch 442/500: Loss: 1.7959 | Train Acc: 43.126% (19105/44300)\n",
      "Epoch 0 - Batch 443/500: Loss: 1.7950 | Train Acc: 43.133% (19151/44400)\n",
      "Epoch 0 - Batch 444/500: Loss: 1.7940 | Train Acc: 43.144% (19199/44500)\n",
      "Epoch 0 - Batch 445/500: Loss: 1.7928 | Train Acc: 43.175% (19256/44600)\n",
      "Epoch 0 - Batch 446/500: Loss: 1.7918 | Train Acc: 43.195% (19308/44700)\n",
      "Epoch 0 - Batch 447/500: Loss: 1.7910 | Train Acc: 43.214% (19360/44800)\n",
      "Epoch 0 - Batch 448/500: Loss: 1.7898 | Train Acc: 43.234% (19412/44900)\n",
      "Epoch 0 - Batch 449/500: Loss: 1.7886 | Train Acc: 43.258% (19466/45000)\n",
      "Epoch 0 - Batch 450/500: Loss: 1.7877 | Train Acc: 43.271% (19515/45100)\n",
      "Epoch 0 - Batch 451/500: Loss: 1.7865 | Train Acc: 43.299% (19571/45200)\n",
      "Epoch 0 - Batch 452/500: Loss: 1.7857 | Train Acc: 43.296% (19613/45300)\n",
      "Epoch 0 - Batch 453/500: Loss: 1.7847 | Train Acc: 43.317% (19666/45400)\n",
      "Epoch 0 - Batch 454/500: Loss: 1.7836 | Train Acc: 43.347% (19723/45500)\n",
      "Epoch 0 - Batch 455/500: Loss: 1.7828 | Train Acc: 43.362% (19773/45600)\n",
      "Epoch 0 - Batch 456/500: Loss: 1.7824 | Train Acc: 43.363% (19817/45700)\n",
      "Epoch 0 - Batch 457/500: Loss: 1.7813 | Train Acc: 43.382% (19869/45800)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Batch 458/500: Loss: 1.7806 | Train Acc: 43.388% (19915/45900)\n",
      "Epoch 0 - Batch 459/500: Loss: 1.7795 | Train Acc: 43.402% (19965/46000)\n",
      "Epoch 0 - Batch 460/500: Loss: 1.7785 | Train Acc: 43.412% (20013/46100)\n",
      "Epoch 0 - Batch 461/500: Loss: 1.7776 | Train Acc: 43.424% (20062/46200)\n",
      "Epoch 0 - Batch 462/500: Loss: 1.7770 | Train Acc: 43.432% (20109/46300)\n",
      "Epoch 0 - Batch 463/500: Loss: 1.7764 | Train Acc: 43.438% (20155/46400)\n",
      "Epoch 0 - Batch 464/500: Loss: 1.7752 | Train Acc: 43.460% (20209/46500)\n",
      "Epoch 0 - Batch 465/500: Loss: 1.7742 | Train Acc: 43.474% (20259/46600)\n",
      "Epoch 0 - Batch 466/500: Loss: 1.7735 | Train Acc: 43.486% (20308/46700)\n",
      "Epoch 0 - Batch 467/500: Loss: 1.7727 | Train Acc: 43.496% (20356/46800)\n",
      "Epoch 0 - Batch 468/500: Loss: 1.7717 | Train Acc: 43.501% (20402/46900)\n",
      "Epoch 0 - Batch 469/500: Loss: 1.7705 | Train Acc: 43.530% (20459/47000)\n",
      "Epoch 0 - Batch 470/500: Loss: 1.7695 | Train Acc: 43.548% (20511/47100)\n",
      "Epoch 0 - Batch 471/500: Loss: 1.7688 | Train Acc: 43.555% (20558/47200)\n",
      "Epoch 0 - Batch 472/500: Loss: 1.7673 | Train Acc: 43.592% (20619/47300)\n",
      "Epoch 0 - Batch 473/500: Loss: 1.7662 | Train Acc: 43.608% (20670/47400)\n",
      "Epoch 0 - Batch 474/500: Loss: 1.7651 | Train Acc: 43.629% (20724/47500)\n",
      "Epoch 0 - Batch 475/500: Loss: 1.7637 | Train Acc: 43.666% (20785/47600)\n",
      "Epoch 0 - Batch 476/500: Loss: 1.7629 | Train Acc: 43.677% (20834/47700)\n",
      "Epoch 0 - Batch 477/500: Loss: 1.7620 | Train Acc: 43.676% (20877/47800)\n",
      "Epoch 0 - Batch 478/500: Loss: 1.7608 | Train Acc: 43.699% (20932/47900)\n",
      "Epoch 0 - Batch 479/500: Loss: 1.7597 | Train Acc: 43.725% (20988/48000)\n",
      "Epoch 0 - Batch 480/500: Loss: 1.7588 | Train Acc: 43.757% (21047/48100)\n",
      "Epoch 0 - Batch 481/500: Loss: 1.7578 | Train Acc: 43.770% (21097/48200)\n",
      "Epoch 0 - Batch 482/500: Loss: 1.7569 | Train Acc: 43.793% (21152/48300)\n",
      "Epoch 0 - Batch 483/500: Loss: 1.7562 | Train Acc: 43.810% (21204/48400)\n",
      "Epoch 0 - Batch 484/500: Loss: 1.7550 | Train Acc: 43.841% (21263/48500)\n",
      "Epoch 0 - Batch 485/500: Loss: 1.7543 | Train Acc: 43.848% (21310/48600)\n",
      "Epoch 0 - Batch 486/500: Loss: 1.7531 | Train Acc: 43.867% (21363/48700)\n",
      "Epoch 0 - Batch 487/500: Loss: 1.7521 | Train Acc: 43.893% (21420/48800)\n",
      "Epoch 0 - Batch 488/500: Loss: 1.7512 | Train Acc: 43.908% (21471/48900)\n",
      "Epoch 0 - Batch 489/500: Loss: 1.7505 | Train Acc: 43.916% (21519/49000)\n",
      "Epoch 0 - Batch 490/500: Loss: 1.7495 | Train Acc: 43.947% (21578/49100)\n",
      "Epoch 0 - Batch 491/500: Loss: 1.7489 | Train Acc: 43.961% (21629/49200)\n",
      "Epoch 0 - Batch 492/500: Loss: 1.7477 | Train Acc: 43.980% (21682/49300)\n",
      "Epoch 0 - Batch 493/500: Loss: 1.7468 | Train Acc: 43.998% (21735/49400)\n",
      "Epoch 0 - Batch 494/500: Loss: 1.7455 | Train Acc: 44.030% (21795/49500)\n",
      "Epoch 0 - Batch 495/500: Loss: 1.7444 | Train Acc: 44.058% (21853/49600)\n",
      "Epoch 0 - Batch 496/500: Loss: 1.7434 | Train Acc: 44.099% (21917/49700)\n",
      "Epoch 0 - Batch 497/500: Loss: 1.7424 | Train Acc: 44.122% (21973/49800)\n",
      "Epoch 0 - Batch 498/500: Loss: 1.7414 | Train Acc: 44.140% (22026/49900)\n",
      "Epoch 0 - Batch 499/500: Loss: 1.7407 | Train Acc: 44.138% (22069/50000)\n",
      "Evaluating: Batch 0/100: Loss: 1.2177 | Test Acc: 61.000% (61/100)\n",
      "Evaluating: Batch 1/100: Loss: 0.6832 | Test Acc: 56.500% (113/200)\n",
      "Evaluating: Batch 2/100: Loss: 0.4701 | Test Acc: 54.333% (163/300)\n",
      "Evaluating: Batch 3/100: Loss: 0.3291 | Test Acc: 53.750% (215/400)\n",
      "Evaluating: Batch 4/100: Loss: 0.2733 | Test Acc: 52.400% (262/500)\n",
      "Evaluating: Batch 5/100: Loss: 0.1779 | Test Acc: 53.000% (318/600)\n",
      "Evaluating: Batch 6/100: Loss: 0.1718 | Test Acc: 52.571% (368/700)\n",
      "Evaluating: Batch 7/100: Loss: 0.1890 | Test Acc: 52.000% (416/800)\n",
      "Evaluating: Batch 8/100: Loss: 0.1428 | Test Acc: 52.000% (468/900)\n",
      "Evaluating: Batch 9/100: Loss: 0.1206 | Test Acc: 52.200% (522/1000)\n",
      "Evaluating: Batch 10/100: Loss: 0.1077 | Test Acc: 52.727% (580/1100)\n",
      "Evaluating: Batch 11/100: Loss: 0.1004 | Test Acc: 53.000% (636/1200)\n",
      "Evaluating: Batch 12/100: Loss: 0.1054 | Test Acc: 52.308% (680/1300)\n",
      "Evaluating: Batch 13/100: Loss: 0.1022 | Test Acc: 51.714% (724/1400)\n",
      "Evaluating: Batch 14/100: Loss: 0.0821 | Test Acc: 52.200% (783/1500)\n",
      "Evaluating: Batch 15/100: Loss: 0.1060 | Test Acc: 51.500% (824/1600)\n",
      "Evaluating: Batch 16/100: Loss: 0.0695 | Test Acc: 51.765% (880/1700)\n",
      "Evaluating: Batch 17/100: Loss: 0.0819 | Test Acc: 51.333% (924/1800)\n",
      "Evaluating: Batch 18/100: Loss: 0.0658 | Test Acc: 51.947% (987/1900)\n",
      "Evaluating: Batch 19/100: Loss: 0.0735 | Test Acc: 51.650% (1033/2000)\n",
      "Evaluating: Batch 20/100: Loss: 0.0585 | Test Acc: 51.619% (1084/2100)\n",
      "Evaluating: Batch 21/100: Loss: 0.0609 | Test Acc: 51.727% (1138/2200)\n",
      "Evaluating: Batch 22/100: Loss: 0.0559 | Test Acc: 51.739% (1190/2300)\n",
      "Evaluating: Batch 23/100: Loss: 0.0555 | Test Acc: 51.833% (1244/2400)\n",
      "Evaluating: Batch 24/100: Loss: 0.0531 | Test Acc: 51.920% (1298/2500)\n",
      "Evaluating: Batch 25/100: Loss: 0.0699 | Test Acc: 51.423% (1337/2600)\n",
      "Evaluating: Batch 26/100: Loss: 0.0450 | Test Acc: 51.519% (1391/2700)\n",
      "Evaluating: Batch 27/100: Loss: 0.0482 | Test Acc: 51.571% (1444/2800)\n",
      "Evaluating: Batch 28/100: Loss: 0.0470 | Test Acc: 51.586% (1496/2900)\n",
      "Evaluating: Batch 29/100: Loss: 0.0447 | Test Acc: 51.800% (1554/3000)\n",
      "Evaluating: Batch 30/100: Loss: 0.0431 | Test Acc: 51.839% (1607/3100)\n",
      "Evaluating: Batch 31/100: Loss: 0.0346 | Test Acc: 52.219% (1671/3200)\n",
      "Evaluating: Batch 32/100: Loss: 0.0372 | Test Acc: 52.364% (1728/3300)\n",
      "Evaluating: Batch 33/100: Loss: 0.0418 | Test Acc: 52.382% (1781/3400)\n",
      "Evaluating: Batch 34/100: Loss: 0.0402 | Test Acc: 52.257% (1829/3500)\n",
      "Evaluating: Batch 35/100: Loss: 0.0369 | Test Acc: 52.083% (1875/3600)\n",
      "Evaluating: Batch 36/100: Loss: 0.0395 | Test Acc: 52.054% (1926/3700)\n",
      "Evaluating: Batch 37/100: Loss: 0.0358 | Test Acc: 52.053% (1978/3800)\n",
      "Evaluating: Batch 38/100: Loss: 0.0296 | Test Acc: 52.051% (2030/3900)\n",
      "Evaluating: Batch 39/100: Loss: 0.0358 | Test Acc: 51.925% (2077/4000)\n",
      "Evaluating: Batch 40/100: Loss: 0.0332 | Test Acc: 51.732% (2121/4100)\n",
      "Evaluating: Batch 41/100: Loss: 0.0316 | Test Acc: 51.690% (2171/4200)\n",
      "Evaluating: Batch 42/100: Loss: 0.0303 | Test Acc: 51.744% (2225/4300)\n",
      "Evaluating: Batch 43/100: Loss: 0.0293 | Test Acc: 51.818% (2280/4400)\n",
      "Evaluating: Batch 44/100: Loss: 0.0296 | Test Acc: 51.867% (2334/4500)\n",
      "Evaluating: Batch 45/100: Loss: 0.0301 | Test Acc: 51.935% (2389/4600)\n",
      "Evaluating: Batch 46/100: Loss: 0.0289 | Test Acc: 51.915% (2440/4700)\n",
      "Evaluating: Batch 47/100: Loss: 0.0281 | Test Acc: 51.938% (2493/4800)\n",
      "Evaluating: Batch 48/100: Loss: 0.0240 | Test Acc: 51.980% (2547/4900)\n",
      "Evaluating: Batch 49/100: Loss: 0.0264 | Test Acc: 51.900% (2595/5000)\n",
      "Evaluating: Batch 50/100: Loss: 0.0241 | Test Acc: 52.000% (2652/5100)\n",
      "Evaluating: Batch 51/100: Loss: 0.0273 | Test Acc: 51.962% (2702/5200)\n",
      "Evaluating: Batch 52/100: Loss: 0.0276 | Test Acc: 51.830% (2747/5300)\n",
      "Evaluating: Batch 53/100: Loss: 0.0266 | Test Acc: 51.778% (2796/5400)\n",
      "Evaluating: Batch 54/100: Loss: 0.0260 | Test Acc: 51.673% (2842/5500)\n",
      "Evaluating: Batch 55/100: Loss: 0.0258 | Test Acc: 51.679% (2894/5600)\n",
      "Evaluating: Batch 56/100: Loss: 0.0253 | Test Acc: 51.684% (2946/5700)\n",
      "Evaluating: Batch 57/100: Loss: 0.0188 | Test Acc: 51.810% (3005/5800)\n",
      "Evaluating: Batch 58/100: Loss: 0.0232 | Test Acc: 51.712% (3051/5900)\n",
      "Evaluating: Batch 59/100: Loss: 0.0226 | Test Acc: 51.700% (3102/6000)\n",
      "Evaluating: Batch 60/100: Loss: 0.0202 | Test Acc: 51.607% (3148/6100)\n",
      "Evaluating: Batch 61/100: Loss: 0.0208 | Test Acc: 51.516% (3194/6200)\n",
      "Evaluating: Batch 62/100: Loss: 0.0217 | Test Acc: 51.508% (3245/6300)\n",
      "Evaluating: Batch 63/100: Loss: 0.0201 | Test Acc: 51.578% (3301/6400)\n",
      "Evaluating: Batch 64/100: Loss: 0.0221 | Test Acc: 51.554% (3351/6500)\n",
      "Evaluating: Batch 65/100: Loss: 0.0216 | Test Acc: 51.439% (3395/6600)\n",
      "Evaluating: Batch 66/100: Loss: 0.0214 | Test Acc: 51.373% (3442/6700)\n",
      "Evaluating: Batch 67/100: Loss: 0.0194 | Test Acc: 51.426% (3497/6800)\n",
      "Evaluating: Batch 68/100: Loss: 0.0216 | Test Acc: 51.391% (3546/6900)\n",
      "Evaluating: Batch 69/100: Loss: 0.0213 | Test Acc: 51.286% (3590/7000)\n",
      "Evaluating: Batch 70/100: Loss: 0.0197 | Test Acc: 51.282% (3641/7100)\n",
      "Evaluating: Batch 71/100: Loss: 0.0174 | Test Acc: 51.319% (3695/7200)\n",
      "Evaluating: Batch 72/100: Loss: 0.0175 | Test Acc: 51.274% (3743/7300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 73/100: Loss: 0.0163 | Test Acc: 51.311% (3797/7400)\n",
      "Evaluating: Batch 74/100: Loss: 0.0170 | Test Acc: 51.320% (3849/7500)\n",
      "Evaluating: Batch 75/100: Loss: 0.0163 | Test Acc: 51.368% (3904/7600)\n",
      "Evaluating: Batch 76/100: Loss: 0.0163 | Test Acc: 51.377% (3956/7700)\n",
      "Evaluating: Batch 77/100: Loss: 0.0171 | Test Acc: 51.372% (4007/7800)\n",
      "Evaluating: Batch 78/100: Loss: 0.0179 | Test Acc: 51.380% (4059/7900)\n",
      "Evaluating: Batch 79/100: Loss: 0.0170 | Test Acc: 51.425% (4114/8000)\n",
      "Evaluating: Batch 80/100: Loss: 0.0176 | Test Acc: 51.432% (4166/8100)\n",
      "Evaluating: Batch 81/100: Loss: 0.0174 | Test Acc: 51.439% (4218/8200)\n",
      "Evaluating: Batch 82/100: Loss: 0.0172 | Test Acc: 51.373% (4264/8300)\n",
      "Evaluating: Batch 83/100: Loss: 0.0177 | Test Acc: 51.298% (4309/8400)\n",
      "Evaluating: Batch 84/100: Loss: 0.0166 | Test Acc: 51.224% (4354/8500)\n",
      "Evaluating: Batch 85/100: Loss: 0.0143 | Test Acc: 51.267% (4409/8600)\n",
      "Evaluating: Batch 86/100: Loss: 0.0150 | Test Acc: 51.276% (4461/8700)\n",
      "Evaluating: Batch 87/100: Loss: 0.0149 | Test Acc: 51.318% (4516/8800)\n",
      "Evaluating: Batch 88/100: Loss: 0.0158 | Test Acc: 51.326% (4568/8900)\n",
      "Evaluating: Batch 89/100: Loss: 0.0134 | Test Acc: 51.444% (4630/9000)\n",
      "Evaluating: Batch 90/100: Loss: 0.0152 | Test Acc: 51.451% (4682/9100)\n",
      "Evaluating: Batch 91/100: Loss: 0.0142 | Test Acc: 51.446% (4733/9200)\n",
      "Evaluating: Batch 92/100: Loss: 0.0125 | Test Acc: 51.527% (4792/9300)\n",
      "Evaluating: Batch 93/100: Loss: 0.0141 | Test Acc: 51.500% (4841/9400)\n",
      "Evaluating: Batch 94/100: Loss: 0.0143 | Test Acc: 51.526% (4895/9500)\n",
      "Evaluating: Batch 95/100: Loss: 0.0132 | Test Acc: 51.573% (4951/9600)\n",
      "Evaluating: Batch 96/100: Loss: 0.0146 | Test Acc: 51.577% (5003/9700)\n",
      "Evaluating: Batch 97/100: Loss: 0.0141 | Test Acc: 51.510% (5048/9800)\n",
      "Evaluating: Batch 98/100: Loss: 0.0152 | Test Acc: 51.394% (5088/9900)\n",
      "Evaluating: Batch 99/100: Loss: 0.0142 | Test Acc: 51.340% (5134/10000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--- set up ---\n",
    "if __name__=='__main__':\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(\"CUDA\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"CPU\")\n",
    "\n",
    "    model = CNN().to(device)\n",
    "\n",
    "    # WRITE CODE HERE\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    #--- training ---\n",
    "    for epoch in range(1):\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "        for batch_num, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # WRITE CODE HERE\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            loss = loss_function(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss\n",
    "            total += BATCH_SIZE_TRAIN\n",
    "            \n",
    "            _, predicted = torch.max(output.data,1)\n",
    "            correct = (predicted == target).sum()\n",
    "            train_correct += correct\n",
    "            \n",
    "            \n",
    "\n",
    "            print('Epoch %d - Batch %d/%d: Loss: %.4f | Train Acc: %.3f%% (%d/%d)' % (epoch, batch_num, len(train_loader), train_loss / (batch_num + 1), 100. * train_correct / total, train_correct, total))\n",
    "    #--- test ---\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # WRITE CODE HERE\n",
    "            output = model(data)\n",
    "            test_loss = loss_function(output,target)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            test_correct += (predicted == target).sum() \n",
    "            print('Evaluating: Batch %d/%d: Loss: %.4f | Test Acc: %.3f%% (%d/%d)' % (batch_num, len(test_loader), test_loss / (batch_num + 1), 100. * test_correct / total, test_correct, total))\n",
    "            \n",
    "\n",
    "    # WRITE CODE HERE\n",
    "    #visualize weights for the first conv layer\n",
    "    filters = model.modules()\n",
    "    model_layers = [i for i in model.children()]\n",
    "    first_layer = model_layers[0]\n",
    "    conv_layer = first_layer[0]\n",
    "    plot_weights(conv_layer.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
